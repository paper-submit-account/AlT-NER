{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from wiser.data.dataset_readers import LaptopsDatasetReader\n",
    "from wiser.rules import TaggingRule, LinkingRule, DictionaryMatcher\n",
    "from wiser.generative import get_label_to_ix, get_rules\n",
    "from labelmodels import *\n",
    "from wiser.generative import train_generative_model\n",
    "from labelmodels import LearningConfig\n",
    "from wiser.generative import evaluate_generative_model\n",
    "from wiser.data import save_label_distribution\n",
    "from wiser.rules import ElmoLinkingRule\n",
    "from wiser.eval import *\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tokenizations import get_alignments, get_original_spans\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "from transformers import *\n",
    "from xml.etree import ElementTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94bedccd50e743cda3d091d31dc0458e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='reading instances'), FloatProgress(value=1.0, bar_style='info', layout=Layout(width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd2a4562bfe24863b5a9e594db19a9f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3045.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed99ae0385a0439291095a4ee756535b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='reading instances'), FloatProgress(value=1.0, bar_style='info', layout=Layout(width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56d9412ddf9546e3ad7018b67b22c7b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=800.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "root = \"../data/\"\n",
    "reader = LaptopsDatasetReader()\n",
    "train_data = reader.read(root + 'LaptopReview/Laptop_Train_v2.xml')\n",
    "test_data = reader.read(root + 'LaptopReview/Laptops_Test_Data_phaseB.xml')\n",
    "\n",
    "laptops_docs = train_data + test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd1522997c884ff6835a587400d9ca1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3845.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f7e7a92db95498fb130b0b76b9cd932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3845.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20e9d1fa5d9a4529a1603eba460c60d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3845.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca4f7d8bd3b42fdb9a90a13640e7480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3845.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82085ebd10074bb0a9549b80b2aa085f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3845.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc9eade98a5c415a86572ab4fd0b8fd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3845.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09e9abb3aa3143b28bf192b9d16ccfc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3845.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "590a94c8d36d4966963194bccaa7844e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3845.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cae86b12e0c41a58fa3a22478204e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3845.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac798187ccdd4fcb96b3a8bef8db94ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3845.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3830202ffb8d433185b05bf313e30dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3845.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "885e871454be457f82d8a88f8c7376a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3845.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dict_core = set()\n",
    "with open(root + 'AutoNER_dicts/LaptopReview/dict_core.txt') as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip().split()\n",
    "        term = tuple(line[1:])\n",
    "        dict_core.add(term)\n",
    "\n",
    "\n",
    "dict_full = set()\n",
    "\n",
    "with open(root + 'AutoNER_dicts/LaptopReview/dict_full.txt') as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip().split()\n",
    "        if len(line) > 1:\n",
    "            dict_full.add(tuple(line))\n",
    "\n",
    "lf = DictionaryMatcher(\"CoreDictionary\", dict_core, uncased=True, i_label=\"I\")\n",
    "lf.apply(laptops_docs)\n",
    "\n",
    "other_terms = [['BIOS'], ['color'], ['cord'], ['hinge'], ['hinges'],\n",
    "               ['port'], ['speaker']]\n",
    "lf = DictionaryMatcher(\"OtherTerms\", other_terms, uncased=True, i_label=\"I\")\n",
    "lf.apply(laptops_docs)\n",
    "\n",
    "\n",
    "class ReplaceThe(TaggingRule):\n",
    "    def apply_instance(self, instance):\n",
    "        tokens = [token.text for token in instance['tokens']]\n",
    "        labels = ['ABS'] * len(tokens)\n",
    "\n",
    "        for i in range(len(tokens) - 2):\n",
    "            if tokens[i].lower() == 'replace' and tokens[i +\n",
    "                                                         1].lower() == 'the':\n",
    "                if instance['tokens'][i + 2].pos_ == \"NOUN\":\n",
    "                    labels[i] = 'O'\n",
    "                    labels[i + 1] = 'O'\n",
    "                    labels[i + 2] = 'I'\n",
    "\n",
    "        return labels\n",
    "\n",
    "\n",
    "lf = ReplaceThe()\n",
    "lf.apply(laptops_docs)\n",
    "\n",
    "\n",
    "class iStuff(TaggingRule):\n",
    "    def apply_instance(self, instance):\n",
    "        tokens = [token.text for token in instance['tokens']]\n",
    "        labels = ['ABS'] * len(tokens)\n",
    "\n",
    "        for i in range(len(tokens)):\n",
    "            if len(\n",
    "                    tokens[i]) > 1 and tokens[i][0] == 'i' and tokens[i][1].isupper():\n",
    "                labels[i] = 'I'\n",
    "\n",
    "        return labels\n",
    "\n",
    "\n",
    "lf = iStuff()\n",
    "lf.apply(laptops_docs)\n",
    "\n",
    "\n",
    "class Feelings(TaggingRule):\n",
    "    feeling_words = {\"like\", \"liked\", \"love\", \"dislike\", \"hate\"}\n",
    "\n",
    "    def apply_instance(self, instance):\n",
    "        tokens = [token.text for token in instance['tokens']]\n",
    "        labels = ['ABS'] * len(tokens)\n",
    "\n",
    "        for i in range(len(tokens) - 2):\n",
    "            if tokens[i].lower() in self.feeling_words and tokens[i +\n",
    "                                                                  1].lower() == 'the':\n",
    "                if instance['tokens'][i + 2].pos_ == \"NOUN\":\n",
    "                    labels[i] = 'O'\n",
    "                    labels[i + 1] = 'O'\n",
    "                    labels[i + 2] = 'I'\n",
    "\n",
    "        return labels\n",
    "\n",
    "\n",
    "lf = Feelings()\n",
    "lf.apply(laptops_docs)\n",
    "\n",
    "\n",
    "class ProblemWithThe(TaggingRule):\n",
    "    def apply_instance(self, instance):\n",
    "        tokens = [token.text for token in instance['tokens']]\n",
    "        labels = ['ABS'] * len(tokens)\n",
    "\n",
    "        for i in range(len(tokens) - 3):\n",
    "            if tokens[i].lower() == 'problem' and tokens[i + \\\n",
    "                               1].lower() == 'with' and tokens[i + 2].lower() == 'the':\n",
    "                if instance['tokens'][i + 3].pos_ == \"NOUN\":\n",
    "                    labels[i] = 'O'\n",
    "                    labels[i + 1] = 'O'\n",
    "                    labels[i + 2] = 'O'\n",
    "                    labels[i + 3] = 'I'\n",
    "\n",
    "        return labels\n",
    "\n",
    "\n",
    "lf = ProblemWithThe()\n",
    "lf.apply(laptops_docs)\n",
    "\n",
    "\n",
    "class External(TaggingRule):\n",
    "    def apply_instance(self, instance):\n",
    "        tokens = [token.text for token in instance['tokens']]\n",
    "        labels = ['ABS'] * len(tokens)\n",
    "\n",
    "        for i in range(len(tokens) - 1):\n",
    "            if tokens[i].lower() == 'external':\n",
    "                labels[i] = 'I'\n",
    "                labels[i + 1] = 'I'\n",
    "\n",
    "        return labels\n",
    "\n",
    "\n",
    "lf = External()\n",
    "lf.apply(laptops_docs)\n",
    "\n",
    "\n",
    "stop_words = {\"a\", \"and\", \"as\", \"be\", \"but\", \"do\", \"even\",\n",
    "              \"for\", \"from\",\n",
    "              \"had\", \"has\", \"have\", \"i\", \"in\", \"is\", \"its\", \"just\",\n",
    "              \"my\", \"no\", \"not\", \"of\", \"on\", \"or\",\n",
    "              \"that\", \"the\", \"these\", \"this\", \"those\", \"to\", \"very\",\n",
    "              \"what\", \"which\", \"who\", \"with\"}\n",
    "\n",
    "\n",
    "class StopWords(TaggingRule):\n",
    "\n",
    "    def apply_instance(self, instance):\n",
    "        labels = ['ABS'] * len(instance['tokens'])\n",
    "\n",
    "        for i in range(len(instance['tokens'])):\n",
    "            if instance['tokens'][i].lemma_ in stop_words:\n",
    "                labels[i] = 'O'\n",
    "        return labels\n",
    "\n",
    "\n",
    "lf = StopWords()\n",
    "lf.apply(laptops_docs)\n",
    "\n",
    "\n",
    "class Punctuation(TaggingRule):\n",
    "    pos = {\"PUNCT\"}\n",
    "\n",
    "    def apply_instance(self, instance):\n",
    "        labels = ['ABS'] * len(instance['tokens'])\n",
    "\n",
    "        for i, pos in enumerate([token.pos_ for token in instance['tokens']]):\n",
    "            if pos in self.pos:\n",
    "                labels[i] = 'O'\n",
    "\n",
    "        return labels\n",
    "\n",
    "\n",
    "lf = Punctuation()\n",
    "lf.apply(laptops_docs)\n",
    "\n",
    "\n",
    "class Pronouns(TaggingRule):\n",
    "    pos = {\"PRON\"}\n",
    "\n",
    "    def apply_instance(self, instance):\n",
    "        labels = ['ABS'] * len(instance['tokens'])\n",
    "\n",
    "        for i, pos in enumerate([token.pos_ for token in instance['tokens']]):\n",
    "            if pos in self.pos:\n",
    "                labels[i] = 'O'\n",
    "\n",
    "        return labels\n",
    "\n",
    "\n",
    "lf = Pronouns()\n",
    "lf.apply(laptops_docs)\n",
    "\n",
    "\n",
    "class NotFeatures(TaggingRule):\n",
    "    keywords = {\"laptop\", \"computer\", \"pc\"}\n",
    "\n",
    "    def apply_instance(self, instance):\n",
    "        labels = ['ABS'] * len(instance['tokens'])\n",
    "\n",
    "        for i in range(len(instance['tokens'])):\n",
    "            if instance['tokens'][i].lemma_ in self.keywords:\n",
    "                labels[i] = 'O'\n",
    "        return labels\n",
    "\n",
    "\n",
    "lf = NotFeatures()\n",
    "lf.apply(laptops_docs)\n",
    "\n",
    "\n",
    "class Adv(TaggingRule):\n",
    "    pos = {\"ADV\"}\n",
    "\n",
    "    def apply_instance(self, instance):\n",
    "        labels = ['ABS'] * len(instance['tokens'])\n",
    "\n",
    "        for i, pos in enumerate([token.pos_ for token in instance['tokens']]):\n",
    "            if pos in self.pos:\n",
    "                labels[i] = 'O'\n",
    "\n",
    "        return labels\n",
    "\n",
    "\n",
    "lf = Adv()\n",
    "lf.apply(laptops_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linking functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95a7866690654963b63053de6ae8026d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3845.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b655f6d3ec7747d1b73b5dbb7f1c5fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3845.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f807aa4e0a364f47a3c23954d097da31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3845.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb16f12900fc4a7f9f836386fbe48910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3845.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class CompoundPhrase(LinkingRule):\n",
    "    def apply_instance(self, instance):\n",
    "        links = [0] * len(instance['tokens'])\n",
    "        for i in range(1, len(instance['tokens'])):\n",
    "            if instance['tokens'][i - 1].dep_ == \"compound\":\n",
    "                links[i] = 1\n",
    "\n",
    "        return links\n",
    "\n",
    "\n",
    "lf = CompoundPhrase()\n",
    "lf.apply(laptops_docs)\n",
    "\n",
    "\n",
    "lf = ElmoLinkingRule(.8)\n",
    "lf.apply(laptops_docs)\n",
    "\n",
    "\n",
    "class ExtractedPhrase(LinkingRule):\n",
    "    def __init__(self, terms):\n",
    "        self.term_dict = {}\n",
    "\n",
    "        for term in terms:\n",
    "            term = [token.lower() for token in term]\n",
    "            if term[0] not in self.term_dict:\n",
    "                self.term_dict[term[0]] = []\n",
    "            self.term_dict[term[0]].append(term)\n",
    "\n",
    "        # Sorts the terms in decreasing order so that we match the longest\n",
    "        # first\n",
    "        for first_token in self.term_dict.keys():\n",
    "            to_sort = self.term_dict[first_token]\n",
    "            self.term_dict[first_token] = sorted(\n",
    "                to_sort, reverse=True, key=lambda x: len(x))\n",
    "\n",
    "    def apply_instance(self, instance):\n",
    "        tokens = [token.text.lower() for token in instance['tokens']]\n",
    "        links = [0] * len(instance['tokens'])\n",
    "\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            if tokens[i] in self.term_dict:\n",
    "                candidates = self.term_dict[tokens[i]]\n",
    "                for c in candidates:\n",
    "                    # Checks whether normalized AllenNLP tokens equal the list\n",
    "                    # of string tokens defining the term in the dictionary\n",
    "                    if i + len(c) <= len(tokens):\n",
    "                        equal = True\n",
    "                        for j in range(len(c)):\n",
    "                            if tokens[i + j] != c[j]:\n",
    "                                equal = False\n",
    "                                break\n",
    "\n",
    "                        # If tokens match, labels the instance tokens\n",
    "                        if equal:\n",
    "                            for j in range(i + 1, i + len(c)):\n",
    "                                links[j] = 1\n",
    "                            i = i + len(c) - 1\n",
    "                            break\n",
    "            i += 1\n",
    "\n",
    "        return links\n",
    "\n",
    "\n",
    "lf = ExtractedPhrase(dict_full)\n",
    "lf.apply(laptops_docs)\n",
    "\n",
    "\n",
    "class ConsecutiveCapitals(LinkingRule):\n",
    "    def apply_instance(self, instance):\n",
    "        links = [0] * len(instance['tokens'])\n",
    "        # We skip the first pair since the first\n",
    "        # token is almost always capitalized\n",
    "        for i in range(2, len(instance['tokens'])):\n",
    "            # We skip this token if it all capitals\n",
    "            all_caps = True\n",
    "            text = instance['tokens'][i].text\n",
    "            for char in text:\n",
    "                if char.islower():\n",
    "                    all_caps = False\n",
    "                    break\n",
    "\n",
    "            if not all_caps and text[0].isupper(\n",
    "            ) and instance['tokens'][i - 1].text[0].isupper():\n",
    "                links[i] = 1\n",
    "\n",
    "        return links\n",
    "\n",
    "\n",
    "lf = ConsecutiveCapitals()\n",
    "lf.apply(laptops_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_to_token_span(tokens: List[str],\n",
    "                      text: str,\n",
    "                      txt_spans: List[tuple]):\n",
    "    \"\"\"\n",
    "    Transfer text-domain spans to token-domain spans\n",
    "    :param tokens: tokens\n",
    "    :param text: text\n",
    "    :param txt_spans: text spans tuples: (start, end, ...)\n",
    "    :return: a list of transferred span tuples.\n",
    "    \"\"\"\n",
    "    token_indices = get_original_spans(tokens, text)\n",
    "    tgt_spans = list()\n",
    "    for txt_span in txt_spans:\n",
    "        spacy_start = txt_span[0]\n",
    "        spacy_end = txt_span[1]\n",
    "        start = None\n",
    "        end = None\n",
    "        for i, (s, e) in enumerate(token_indices):\n",
    "            if s <= spacy_start < e:\n",
    "                start = i\n",
    "            if s <= spacy_end <= e:\n",
    "                end = i + 1\n",
    "            if (start is not None) and (end is not None):\n",
    "                break\n",
    "        assert (start is not None) and (end is not None), ValueError(\"input spans out of scope\")\n",
    "        tgt_spans.append((start, end))\n",
    "    return tgt_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respan(src_tokens: List[str],\n",
    "           tgt_tokens: List[str],\n",
    "           src_span: List[tuple]):\n",
    "    \"\"\"\n",
    "    transfer original spans to target spans\n",
    "    :param src_tokens: source tokens\n",
    "    :param tgt_tokens: target tokens\n",
    "    :param src_span: a list of span tuples. The first element in the tuple\n",
    "    should be the start index and the second should be the end index\n",
    "    :return: a list of transferred span tuples.\n",
    "    \"\"\"\n",
    "    s2t, _ = get_alignments(src_tokens, tgt_tokens)\n",
    "    tgt_spans = list()\n",
    "    for spans in src_span:\n",
    "        start = s2t[spans[0]][0]\n",
    "        if spans[1] < len(s2t):\n",
    "            end = s2t[spans[1]-1][-1] + 1\n",
    "        else:\n",
    "            end = s2t[-1][-1]\n",
    "        if end == start:\n",
    "            end += 1\n",
    "        tgt_spans.append((start, end))\n",
    "\n",
    "    return tgt_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_span(labels: List[str],\n",
    "                  scheme: Optional[str] = 'BIO') -> dict:\n",
    "    \"\"\"\n",
    "    convert labels to spans\n",
    "    :param labels: a list of labels\n",
    "    :param scheme: labeling scheme, in ['BIO', 'BILOU'].\n",
    "    :return: labeled spans, a list of tuples (start_idx, end_idx, label)\n",
    "    \"\"\"\n",
    "    assert scheme in ['BIO', 'BILOU'], ValueError(\"unknown labeling scheme\")\n",
    "\n",
    "    labeled_spans = dict()\n",
    "    i = 0\n",
    "    while i < len(labels):\n",
    "        if labels[i] == 'O' or labels[i] == 'ABS':\n",
    "            i += 1\n",
    "            continue\n",
    "        else:\n",
    "            if scheme == 'BIO':\n",
    "                if labels[i][0] == 'B':\n",
    "                    start = i\n",
    "                    lb = labels[i][2:]\n",
    "                    i += 1\n",
    "                    try:\n",
    "                        while labels[i][0] == 'I':\n",
    "                            i += 1\n",
    "                        end = i\n",
    "                        labeled_spans[(start, end)] = lb\n",
    "                    except IndexError:\n",
    "                        end = i\n",
    "                        labeled_spans[(start, end)] = lb\n",
    "                        i += 1\n",
    "                # this should not happen\n",
    "                elif labels[i][0] == 'I':\n",
    "                    i += 1\n",
    "            elif scheme == 'BILOU':\n",
    "                if labels[i][0] == 'U':\n",
    "                    start = i\n",
    "                    end = i + 1\n",
    "                    lb = labels[i][2:]\n",
    "                    labeled_spans[(start, end)] = lb\n",
    "                    i += 1\n",
    "                elif labels[i][0] == 'B':\n",
    "                    start = i\n",
    "                    lb = labels[i][2:]\n",
    "                    i += 1\n",
    "                    try:\n",
    "                        while labels[i][0] != 'L':\n",
    "                            i += 1\n",
    "                        end = i\n",
    "                        labeled_spans[(start, end)] = lb\n",
    "                    except IndexError:\n",
    "                        end = i\n",
    "                        labeled_spans[(start, end)] = lb\n",
    "                        break\n",
    "                    i += 1\n",
    "                else:\n",
    "                    i += 1\n",
    "\n",
    "    return labeled_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bert_emb(sents: List[str],\n",
    "                   tokenizer,\n",
    "                   model,\n",
    "                   device: str):\n",
    "    bert_embs = list()\n",
    "    for i, sent in enumerate(sents):\n",
    "\n",
    "        joint_sent = ' '.join(sent)\n",
    "        bert_tokens = tokenizer.tokenize(joint_sent)\n",
    "\n",
    "        input_ids = torch.tensor([tokenizer.encode(joint_sent, add_special_tokens=True)], device=device)\n",
    "        # calculate BERT last layer embeddings\n",
    "        with torch.no_grad():\n",
    "            last_hidden_states = model(input_ids)[0].squeeze(0).to('cpu')\n",
    "            trunc_hidden_states = last_hidden_states[1:-1, :]\n",
    "\n",
    "        ori2bert, bert2ori = get_alignments(sent, bert_tokens)\n",
    "\n",
    "        emb_list = list()\n",
    "        for idx in ori2bert:\n",
    "            emb = trunc_hidden_states[idx, :]\n",
    "            emb_list.append(emb.mean(dim=0))\n",
    "\n",
    "        # TODO: using the embedding of [CLS] may not be the best idea\n",
    "        # It does not matter since that embedding is not used in the training\n",
    "        emb_list = [last_hidden_states[0, :]] + emb_list\n",
    "        bert_emb = torch.stack(emb_list)\n",
    "        bert_embs.append(bert_emb)\n",
    "    return bert_embs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct my dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = 'TERM'\n",
    "LINK = 'LINK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"../data/\"\n",
    "train_path = root + 'LaptopReview/Laptop_Train_v2.xml'\n",
    "test_path = root + 'LaptopReview/Laptops_Test_Data_phaseB.xml'\n",
    "\n",
    "root = ElementTree.parse(train_path).getroot()\n",
    "train_xml_sents = root.findall(\"./sentence\")\n",
    "root = ElementTree.parse(test_path).getroot()\n",
    "test_xml_sents = root.findall(\"./sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list()\n",
    "for xml_sent in train_xml_sents:\n",
    "    text = xml_sent.find(\"text\").text\n",
    "    sentences.append(text)\n",
    "for xml_sent in test_xml_sents:\n",
    "    text = xml_sent.find(\"text\").text\n",
    "    sentences.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_token_list = list()\n",
    "src_anno_list = list()\n",
    "weak_anno_list = list()\n",
    "link_anno_list = list()\n",
    "\n",
    "allen_data = laptops_docs\n",
    "mapping_dict = {0:'O', 1:'I'}\n",
    "\n",
    "for src_txt, allen_annos in zip(sentences, allen_data):\n",
    "    \n",
    "    # handle the data read from the source text\n",
    "    src_tokens = word_tokenize(src_txt)\n",
    "    for i in range(len(src_tokens)):\n",
    "        if src_tokens[i] == r'``' or src_tokens[i] == r\"''\":\n",
    "            src_tokens[i] = r'\"'\n",
    "    \n",
    "    \n",
    "    allen_tokens = list(map(str, allen_annos['tokens']))\n",
    "    \n",
    "    src_labels = list(allen_annos['tags'])\n",
    "    pre_anno = 'O'\n",
    "    for i in range(len(src_labels)):\n",
    "        current_anno = src_labels[i]\n",
    "        if src_labels[i] == 'I':\n",
    "            if pre_anno != 'I':\n",
    "                src_labels[i] = 'B-' + LABEL\n",
    "            else:\n",
    "                src_labels[i] = 'I-' + LABEL\n",
    "        pre_anno = current_anno\n",
    "    src_spans = label_to_span(src_labels)\n",
    "    src_spans = respan(allen_tokens, src_tokens, src_spans)\n",
    "    \n",
    "    src_annos = dict()\n",
    "    for span in src_spans:\n",
    "        src_annos[span] = LABEL\n",
    "\n",
    "    src_token_list.append(src_tokens)\n",
    "    src_anno_list.append(src_annos)\n",
    "    \n",
    "    # handle the data constructed using Allennlp\n",
    "    weak_anno = dict()\n",
    "    \n",
    "    for k in allen_annos['WISER_LABELS']:\n",
    "        std_lbs = allen_annos['WISER_LABELS'][k][:]\n",
    "        \n",
    "        pre_anno = 'O'\n",
    "        for i in range(len(std_lbs)):\n",
    "            current_anno = std_lbs[i]\n",
    "            if std_lbs[i] == 'I':\n",
    "                if pre_anno != 'I':\n",
    "                    std_lbs[i] = 'B-' + LABEL\n",
    "                else:\n",
    "                    std_lbs[i] = 'I-' + LABEL\n",
    "            pre_anno = current_anno\n",
    "        weak_span = label_to_span(std_lbs)\n",
    "\n",
    "        src_weak_span = respan(allen_tokens, src_tokens, weak_span)\n",
    "        src_weak_anno = dict()\n",
    "        for span in src_weak_span:\n",
    "            src_weak_anno[span] = [(LABEL, 1.0)]\n",
    "            \n",
    "        weak_anno[k] = src_weak_anno\n",
    "    weak_anno_list.append(weak_anno)\n",
    "\n",
    "    \n",
    "    linked_dict = dict()\n",
    "    for src, entity_lbs in allen_annos['WISER_LINKS'].items():\n",
    "        entity_lbs = [mapping_dict[lb] for lb in entity_lbs]\n",
    "\n",
    "        pre_anno = 'O'\n",
    "        for i in range(len(entity_lbs)):\n",
    "            current_anno = entity_lbs[i]\n",
    "            if entity_lbs[i] == 'I':\n",
    "                if pre_anno != 'I':\n",
    "                    entity_lbs[i] = 'B-' + LINK\n",
    "                else:\n",
    "                    entity_lbs[i] = 'I-' + LINK\n",
    "            pre_anno = current_anno\n",
    "\n",
    "        entity_spans = label_to_span(entity_lbs)\n",
    "        complete_span = dict()\n",
    "        for (start, end), lb in entity_spans.items():\n",
    "            if start != 0:\n",
    "                start = start - 1\n",
    "            complete_span[(start, end)] = lb\n",
    "        src_link_span = respan(allen_tokens, src_tokens, complete_span)\n",
    "        linked_dict[src] = src_link_span\n",
    "    link_anno_list.append(linked_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_link_anno_list = list()\n",
    "for tag_anno, link_anno in zip(weak_anno_list, link_anno_list):\n",
    "    tag_spans = list()\n",
    "    for src, spans in tag_anno.items():\n",
    "        tag_spans += list(spans.keys())\n",
    "    for i in range(len(tag_spans)):\n",
    "        tag_spans[i] = set(range(tag_spans[i][0], tag_spans[i][1]))\n",
    "    \n",
    "    link_entities = dict()\n",
    "    for src, spans in link_anno.items():\n",
    "        valid_spans = list()\n",
    "        for span in spans:\n",
    "            if span[1] - span[0] == 1:\n",
    "                continue\n",
    "            span_set = set(range(span[0], span[1]))\n",
    "            for tag_span in tag_spans:\n",
    "                if span_set.intersection(tag_span):\n",
    "                    valid_spans.append(span)\n",
    "        valid_anno = {span: [(LABEL, 1.0)] for span in valid_spans}\n",
    "        link_entities[src] = valid_anno\n",
    "    updated_link_anno_list.append(link_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_anno_list = list()\n",
    "for tag_anno, link_anno in zip(weak_anno_list, updated_link_anno_list):\n",
    "    comb_anno = dict()\n",
    "    for k, v in tag_anno.items():\n",
    "        comb_anno[k] = v\n",
    "    for k, v in link_anno.items():\n",
    "        comb_anno[k] = v\n",
    "    combined_anno_list.append(comb_anno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Bert Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class = BertModel\n",
    "tokenizer_class = BertTokenizer\n",
    "# pretrained_model_name = 'bert-base-cased'\n",
    "pretrained_model_name = 'bert-base-uncased'\n",
    "\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_model_name)\n",
    "model = model_class.from_pretrained(pretrained_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = model.to(device)\n",
    "embs = build_bert_emb(src_token_list, tokenizer, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embs[-800:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_token_list = src_token_list[-800:]\n",
    "test_anno_list = combined_anno_list[-800:]\n",
    "test_lb_list = src_anno_list[-800:]\n",
    "test_emb = embs[-800:]\n",
    "\n",
    "indices = np.arange(len(src_token_list)-800)\n",
    "np.random.shuffle(indices)\n",
    "train_partition = (len(src_token_list)-800) * 4 // 5\n",
    "\n",
    "train_token_list = list()\n",
    "train_anno_list = list()\n",
    "train_lb_list = list()\n",
    "train_emb = list()\n",
    "for i in indices[:train_partition]:\n",
    "    train_token_list.append(src_token_list[i])\n",
    "    train_anno_list.append(combined_anno_list[i])\n",
    "    train_lb_list.append(src_anno_list[i])\n",
    "    train_emb.append(embs[i])\n",
    "\n",
    "dev_token_list = list()\n",
    "dev_anno_list = list()\n",
    "dev_lb_list = list()\n",
    "dev_emb = list()\n",
    "for i in indices[train_partition:]:\n",
    "    dev_token_list.append(src_token_list[i])\n",
    "    dev_anno_list.append(combined_anno_list[i])\n",
    "    dev_lb_list.append(src_anno_list[i])\n",
    "    dev_emb.append(embs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = {\n",
    "    \"sentences\": train_token_list,\n",
    "    \"annotations\": train_anno_list,\n",
    "    \"labels\": train_lb_list\n",
    "}\n",
    "\n",
    "torch.save(train_data, f\"Laptop-linked-train.pt\")\n",
    "torch.save(train_emb, f\"Laptop-emb-train.pt\")\n",
    "\n",
    "dev_data = {\n",
    "    \"sentences\": dev_token_list,\n",
    "    \"annotations\": dev_anno_list,\n",
    "    \"labels\": dev_lb_list\n",
    "}\n",
    "\n",
    "torch.save(dev_data, f\"Laptop-linked-dev.pt\")\n",
    "torch.save(dev_emb, f\"Laptop-emb-dev.pt\")\n",
    "\n",
    "test_data = {\n",
    "    \"sentences\": test_token_list,\n",
    "    \"annotations\": test_anno_list,\n",
    "    \"labels\": test_lb_list\n",
    "}\n",
    "\n",
    "torch.save(test_data, f\"Laptop-linked-test.pt\")\n",
    "torch.save(test_emb, f\"Laptop-emb-test.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
