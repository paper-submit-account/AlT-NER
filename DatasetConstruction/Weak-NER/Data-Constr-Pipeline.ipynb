{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "import torch\n",
    "import os\n",
    "from Core.Constants import *\n",
    "from Core import Annotate, IO, Util, Data, HMM\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load data and corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PARTITION = \"dev\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_PARTITION == \"all\":\n",
    "    articles = list()\n",
    "    labels = list()\n",
    "    for name in [\"train\", \"dev\", \"test\"]:\n",
    "        data = torch.load(f'CoNLL03-{name}.pt')\n",
    "        articles += data['documents']\n",
    "        labels += data['labels']\n",
    "else:\n",
    "    data = torch.load(f'CoNLL03-{DATA_PARTITION}.pt')\n",
    "    articles = data['documents']\n",
    "    labels = data['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Use SpaCy to get the weak labels and training priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct SpaCy documents from plain text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "docs = []\n",
    "for sents in tqdm(articles):\n",
    "    doc = Annotate.construct_doc(sents, nlp)\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load annotators and annotate documents with weak labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "united_annotator = Annotate.UnitedAnnotator().add_all()\n",
    "\n",
    "for doc in tqdm(docs):\n",
    "    doc = united_annotator.annotate(doc)\n",
    "torch.save(docs, f\"CoNLL03-SpaCy-{DATA_PARTITION}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract and save training priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = torch.load(f\"CoNLL03-SpaCy-{DATA_PARTITION}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_to_use = [l for l in SOURCE_NAMES if \"conll2003\" not in l]\n",
    "hmm_model = HMM.HMMAnnotator(\n",
    "    sources_to_keep=sources_to_use\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [hmm_model.extract_sequence(doc) for doc in docs]\n",
    "\n",
    "hmm_model._initialise_startprob(x)\n",
    "hmm_model._initialise_transmat(x)\n",
    "hmm_model._initialise_emissions(x)\n",
    "\n",
    "initial_statistics = {\n",
    "    \"state_prior_count\": hmm_model.startprob_prior,\n",
    "    \"state_prior\": hmm_model.startprob_,\n",
    "    \"transition_count\": hmm_model.transmat_prior,\n",
    "    \"transition_matrix\": hmm_model.transmat_,\n",
    "    \"emission_strength\": hmm_model.emission_priors,\n",
    "    \"emission_matrix\": hmm_model.emission_probs\n",
    "}\n",
    "torch.save(initial_statistics, f'CoNLL03-init-stat-{DATA_PARTITION}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Convert SpaCy annotation spans to the original sentence-level spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = torch.load(f\"CoNLL03-SpaCy-{DATA_PARTITION}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list()\n",
    "sent_level_annos = list()\n",
    "for doc, article in zip(docs, articles):\n",
    "    assert len(list(doc.sents)) == len(article)\n",
    "    sentences += article\n",
    "    sent_level_annos += Data.annotate_doc_with_spacy(article, doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_spans = list()\n",
    "for doc_labels in labels:\n",
    "    for sent_labels in doc_labels:\n",
    "        lb_spans.append(Data.label_to_span(sent_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"sentences\": sentences,\n",
    "    \"annotations\": sent_level_annos,\n",
    "    \"labels\": lb_spans,\n",
    "}\n",
    "torch.save(data, f\"Co03-linked-{DATA_PARTITION}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Build BERT embedding for each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class = BertModel\n",
    "tokenizer_class = BertTokenizer\n",
    "# pretrained_model_name = 'bert-base-cased'\n",
    "pretrained_model_name = 'bert-base-uncased'\n",
    "\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_model_name)\n",
    "model = model_class.from_pretrained(pretrained_model_name).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embs = Data.build_bert_emb(sentences, tokenizer, model, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(bert_embs, f\"Co03-emb-{DATA_PARTITION}.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
