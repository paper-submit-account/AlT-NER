{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from wiser.data.dataset_readers import NCBIDiseaseDatasetReader\n",
    "from wiser.rules import TaggingRule, LinkingRule, UMLSMatcher, DictionaryMatcher\n",
    "from wiser.generative import get_label_to_ix, get_rules\n",
    "from labelmodels import *\n",
    "from wiser.generative import train_generative_model\n",
    "from labelmodels import LearningConfig\n",
    "from wiser.generative import evaluate_generative_model\n",
    "from wiser.data import save_label_distribution\n",
    "from wiser.eval import *\n",
    "from wiser.rules import ElmoLinkingRule\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tokenizations import get_alignments, get_original_spans\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Weak Annotations Using Allennlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PARTITION = 'train'\n",
    "\n",
    "file_name = ''\n",
    "if DATA_PARTITION == 'dev':\n",
    "    file_name = 'NCBIdevelopset_corpus.txt'\n",
    "elif DATA_PARTITION == 'test':\n",
    "    file_name = 'NCBItestset_corpus.txt'\n",
    "elif DATA_PARTITION == 'train':\n",
    "    file_name = 'NCBItrainset_corpus.txt'\n",
    "\n",
    "LABEL = 'DISEASE'\n",
    "LINK = 'ENT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "569d437a75064e5792c62ba990a46245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='reading instances'), FloatProgress(value=1.0, bar_style='info', layout=Layout(widthâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9d7c998c8f242819e9cb9c337e53e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=6924.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "reader = NCBIDiseaseDatasetReader()\n",
    "ncbi_docs = reader.read(f'../data/NCBI/{file_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4836f9366e34506b6203c6680157cae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=592.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "992d22997ea340ca8d6882cadff5f1d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=592.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28cf8f4c2fe549f9a91ea060f4d3cdb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=592.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ac218672491431a92aace0cac59e57f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=592.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae658dcf8804ffc9894d506c11396fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=592.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f328c7f71964608a56bf0fc0c98693d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=592.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b35c8fb0ccc04167b6a38dd07d975f7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=592.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "101d7db295ca4324a1ce32cded45edd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=592.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c6605c2ff974ed3aa13f79a82027a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=592.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "914ef041394840ac8688ecaeab7d2c4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=592.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8331487a0e834d849944c438f9c8d502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=592.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46650969b9684742be4fdcc92bc3da37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=592.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd62d4eef9db41ef97d53e5998abca21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=592.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dict_core = set()\n",
    "dict_core_exact = set()\n",
    "with open('../data/AutoNER_dicts/NCBI/dict_core.txt') as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip().split()\n",
    "        term = tuple(line[1:])\n",
    "\n",
    "        if len(term) > 1 or len(term[0]) > 3:\n",
    "            dict_core.add(term)\n",
    "        else:\n",
    "            dict_core_exact.add(term)\n",
    "\n",
    "# Prepends common modifiers\n",
    "to_add = set()\n",
    "for term in dict_core:\n",
    "    to_add.add((\"inherited\", ) + term)\n",
    "    to_add.add((\"Inherited\", ) + term)\n",
    "    to_add.add((\"hereditary\", ) + term)\n",
    "    to_add.add((\"Hereditary\", ) + term)\n",
    "\n",
    "dict_core |= to_add\n",
    "\n",
    "# Removes common FP\n",
    "dict_core_exact.remove((\"WT1\",))\n",
    "dict_core_exact.remove((\"VHL\",))\n",
    "\n",
    "\n",
    "dict_full = set()\n",
    "\n",
    "with open('../data/AutoNER_dicts/NCBI/dict_full.txt') as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip().split()\n",
    "        dict_full.add(tuple(line))\n",
    "\n",
    "\n",
    "lf = DictionaryMatcher(\n",
    "    \"CoreDictionaryUncased\",\n",
    "    dict_core,\n",
    "    uncased=True,\n",
    "    i_label=\"I\")\n",
    "lf.apply(ncbi_docs)\n",
    "\n",
    "\n",
    "lf = DictionaryMatcher(\"CoreDictionaryExact\", dict_core_exact, i_label=\"I\")\n",
    "lf.apply(ncbi_docs)\n",
    "\n",
    "\n",
    "class CancerLike(TaggingRule):\n",
    "    def apply_instance(self, instance):\n",
    "        tokens = [token.text.lower() for token in instance['tokens']]\n",
    "        labels = ['ABS'] * len(tokens)\n",
    "\n",
    "        suffixes = (\"edema\", \"toma\", \"coma\", \"noma\")\n",
    "\n",
    "        for i, token in enumerate(tokens):\n",
    "            for suffix in suffixes:\n",
    "                if token.endswith(suffix) or token.endswith(suffix + \"s\"):\n",
    "                    labels[i] = 'I'\n",
    "        return labels\n",
    "\n",
    "\n",
    "lf = CancerLike()\n",
    "lf.apply(ncbi_docs)\n",
    "\n",
    "\n",
    "class CommonSuffixes(TaggingRule):\n",
    "\n",
    "    suffixes = {\n",
    "        \"agia\",\n",
    "        \"cardia\",\n",
    "        \"trophy\",\n",
    "        \"toxic\",\n",
    "        \"itis\",\n",
    "        \"emia\",\n",
    "        \"pathy\",\n",
    "        \"plasia\"}\n",
    "\n",
    "    def apply_instance(self, instance):\n",
    "        labels = ['ABS'] * len(instance['tokens'])\n",
    "\n",
    "        for i in range(len(instance['tokens'])):\n",
    "            for suffix in self.suffixes:\n",
    "                if instance['tokens'][i].lemma_.endswith(suffix):\n",
    "                    labels[i] = 'I'\n",
    "        return labels\n",
    "\n",
    "\n",
    "lf = CommonSuffixes()\n",
    "lf.apply(ncbi_docs)\n",
    "\n",
    "\n",
    "class Deficiency(TaggingRule):\n",
    "\n",
    "    def apply_instance(self, instance):\n",
    "        labels = ['ABS'] * len(instance['tokens'])\n",
    "\n",
    "        # \"___ deficiency\"\n",
    "        for i in range(len(instance['tokens']) - 1):\n",
    "            if instance['tokens'][i].dep_ == 'compound' and instance['tokens'][i + 1].lemma_ == 'deficiency':\n",
    "                labels[i] = 'I'\n",
    "                labels[i + 1] = 'I'\n",
    "\n",
    "                # Adds any other compound tokens before the phrase\n",
    "                for j in range(i - 1, -1, -1):\n",
    "                    if instance['tokens'][j].dep_ == 'compound':\n",
    "                        labels[j] = 'I'\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "        # \"deficiency of ___\"\n",
    "        for i in range(len(instance['tokens']) - 2):\n",
    "            if instance['tokens'][i].lemma_ == 'deficiency' and instance['tokens'][i + 1].lemma_ == 'of':\n",
    "                labels[i] = 'I'\n",
    "                labels[i + 1] = 'I'\n",
    "                nnp_active = False\n",
    "                for j in range(i + 2, len(instance['tokens'])):\n",
    "                    if instance['tokens'][j].pos_ in ('NOUN', 'PROPN'):\n",
    "                        if not nnp_active:\n",
    "                            nnp_active = True\n",
    "                    elif nnp_active:\n",
    "                        break\n",
    "                    labels[j] = 'I'\n",
    "\n",
    "        return labels\n",
    "\n",
    "\n",
    "lf = Deficiency()\n",
    "lf.apply(ncbi_docs)\n",
    "\n",
    "\n",
    "class Disorder(TaggingRule):\n",
    "\n",
    "    def apply_instance(self, instance):\n",
    "        labels = ['ABS'] * len(instance['tokens'])\n",
    "\n",
    "        for i in range(len(instance['tokens']) - 1):\n",
    "            if instance['tokens'][i].dep_ == 'compound' and instance['tokens'][i +\n",
    "                                                                               1].lemma_ == 'disorder':\n",
    "                labels[i] = 'I'\n",
    "                labels[i + 1] = 'I'\n",
    "\n",
    "                # Adds any other compound tokens before the phrase\n",
    "                for j in range(i - 1, -1, -1):\n",
    "                    if instance['tokens'][j].dep_ == 'compound':\n",
    "                        labels[j] = 'I'\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "        return labels\n",
    "\n",
    "\n",
    "lf = Disorder()\n",
    "lf.apply(ncbi_docs)\n",
    "\n",
    "\n",
    "class Lesion(TaggingRule):\n",
    "\n",
    "    def apply_instance(self, instance):\n",
    "        labels = ['ABS'] * len(instance['tokens'])\n",
    "\n",
    "        for i in range(len(instance['tokens']) - 1):\n",
    "            if instance['tokens'][i].dep_ == 'compound' and instance['tokens'][i +\n",
    "                                                                               1].lemma_ == 'lesion':\n",
    "                labels[i] = 'I'\n",
    "                labels[i + 1] = 'I'\n",
    "\n",
    "                # Adds any other compound tokens before the phrase\n",
    "                for j in range(i - 1, -1, -1):\n",
    "                    if instance['tokens'][j].dep_ == 'compound':\n",
    "                        labels[j] = 'I'\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "        return labels\n",
    "\n",
    "\n",
    "lf = Lesion()\n",
    "lf.apply(ncbi_docs)\n",
    "\n",
    "\n",
    "class Syndrome(TaggingRule):\n",
    "\n",
    "    def apply_instance(self, instance):\n",
    "        labels = ['ABS'] * len(instance['tokens'])\n",
    "\n",
    "        for i in range(len(instance['tokens']) - 1):\n",
    "            if instance['tokens'][i].dep_ == 'compound' and instance['tokens'][i +\n",
    "                                                                               1].lemma_ == 'syndrome':\n",
    "                labels[i] = 'I'\n",
    "                labels[i + 1] = 'I'\n",
    "\n",
    "                # Adds any other compound tokens before the phrase\n",
    "                for j in range(i - 1, -1, -1):\n",
    "                    if instance['tokens'][j].dep_ == 'compound':\n",
    "                        labels[j] = 'I'\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "        return labels\n",
    "\n",
    "\n",
    "lf = Syndrome()\n",
    "lf.apply(ncbi_docs)\n",
    "\n",
    "\n",
    "terms = []\n",
    "with open('../data/umls/umls_body_part.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        terms.append(line.strip().split(\" \"))\n",
    "lf = DictionaryMatcher(\"TEMP\", terms, i_label='TEMP', uncased=True, match_lemmas=True)\n",
    "lf.apply(ncbi_docs)\n",
    "\n",
    "\n",
    "class BodyTerms(TaggingRule):\n",
    "    def apply_instance(self, instance):\n",
    "        tokens = [token.text.lower() for token in instance['tokens']]\n",
    "        labels = ['ABS'] * len(tokens)\n",
    "\n",
    "        terms = {\"cancer\", \"cancers\", \"damage\", \"disease\", \"diseases\", \"pain\", \"injury\", \"injuries\"}\n",
    "\n",
    "        for i in range(0, len(tokens) - 1):\n",
    "            if instance['WISER_LABELS']['TEMP'][i] == 'TEMP':\n",
    "                if tokens[i + 1] in terms:\n",
    "                    labels[i] = \"I\"\n",
    "                    labels[i + 1] = \"I\"\n",
    "        return labels\n",
    "\n",
    "\n",
    "lf = BodyTerms()\n",
    "lf.apply(ncbi_docs)\n",
    "\n",
    "for doc in ncbi_docs:\n",
    "    del doc['WISER_LABELS']['TEMP']\n",
    "\n",
    "\n",
    "class OtherPOS(TaggingRule):\n",
    "    other_pos = {\"ADP\", \"ADV\", \"DET\", \"VERB\"}\n",
    "\n",
    "    def apply_instance(self, instance):\n",
    "        labels = ['ABS'] * len(instance['tokens'])\n",
    "\n",
    "        for i in range(0, len(instance['tokens'])):\n",
    "            if instance['tokens'][i].pos_ in self.other_pos:\n",
    "                labels[i] = \"O\"\n",
    "        return labels\n",
    "\n",
    "\n",
    "lf = OtherPOS()\n",
    "lf.apply(ncbi_docs)\n",
    "\n",
    "\n",
    "stop_words = {\"a\", \"as\", \"be\", \"but\", \"do\", \"even\",\n",
    "              \"for\", \"from\",\n",
    "              \"had\", \"has\", \"have\", \"i\", \"in\", \"is\", \"its\", \"just\",\n",
    "              \"my\", \"no\", \"not\", \"on\", \"or\",\n",
    "              \"that\", \"the\", \"these\", \"this\", \"those\", \"to\", \"very\",\n",
    "              \"what\", \"which\", \"who\", \"with\"}\n",
    "\n",
    "\n",
    "class StopWords(TaggingRule):\n",
    "\n",
    "    def apply_instance(self, instance):\n",
    "        labels = ['ABS'] * len(instance['tokens'])\n",
    "\n",
    "        for i in range(len(instance['tokens'])):\n",
    "            if instance['tokens'][i].lemma_ in stop_words:\n",
    "                labels[i] = 'O'\n",
    "        return labels\n",
    "\n",
    "\n",
    "lf = StopWords()\n",
    "lf.apply(ncbi_docs)\n",
    "\n",
    "\n",
    "class Punctuation(TaggingRule):\n",
    "\n",
    "    other_punc = {\".\", \",\", \"?\", \"!\", \";\", \":\", \"(\", \")\",\n",
    "                  \"%\", \"<\", \">\", \"=\", \"+\", \"/\", \"\\\\\"}\n",
    "\n",
    "    def apply_instance(self, instance):\n",
    "        labels = ['ABS'] * len(instance['tokens'])\n",
    "\n",
    "        for i in range(len(instance['tokens'])):\n",
    "            if instance['tokens'][i].text in self.other_punc:\n",
    "                labels[i] = 'O'\n",
    "        return labels\n",
    "\n",
    "\n",
    "lf = Punctuation()\n",
    "lf.apply(ncbi_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linking Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12313959460143039e789254ad33fe6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=592.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbb85b5341874707b5cfeb77422d5699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=592.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "716a0e65e68e426faea45c08ff938b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=592.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c85346bb47448ca9c4c12736cf58929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=592.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fb2451a2dbc42f497da7c57237a7527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=592.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class PossessivePhrase(LinkingRule):\n",
    "    def apply_instance(self, instance):\n",
    "        links = [0] * len(instance['tokens'])\n",
    "        for i in range(1, len(instance['tokens'])):\n",
    "            if instance['tokens'][i - 1].text == \"'s\" or instance['tokens'][i].text == \"'s\":\n",
    "                links[i] = 1\n",
    "\n",
    "        return links\n",
    "\n",
    "\n",
    "lf = PossessivePhrase()\n",
    "lf.apply(ncbi_docs)\n",
    "\n",
    "\n",
    "class HyphenatedPhrase(LinkingRule):\n",
    "    def apply_instance(self, instance):\n",
    "        links = [0] * len(instance['tokens'])\n",
    "        for i in range(1, len(instance['tokens'])):\n",
    "            if instance['tokens'][i - 1].text == \"-\" or instance['tokens'][i].text == \"-\":\n",
    "                links[i] = 1\n",
    "\n",
    "        return links\n",
    "\n",
    "\n",
    "lf = HyphenatedPhrase()\n",
    "lf.apply(ncbi_docs)\n",
    "\n",
    "\n",
    "lf = ElmoLinkingRule(.8)\n",
    "lf.apply(ncbi_docs)\n",
    "\n",
    "\n",
    "class CommonBigram(LinkingRule):\n",
    "    def apply_instance(self, instance):\n",
    "        links = [0] * len(instance['tokens'])\n",
    "        tokens = [token.text.lower() for token in instance['tokens']]\n",
    "\n",
    "        bigrams = {}\n",
    "        for i in range(1, len(tokens)):\n",
    "            bigram = tokens[i - 1], tokens[i]\n",
    "            if bigram in bigrams:\n",
    "                bigrams[bigram] += 1\n",
    "            else:\n",
    "                bigrams[bigram] = 1\n",
    "\n",
    "        for i in range(1, len(tokens)):\n",
    "            bigram = tokens[i - 1], tokens[i]\n",
    "            count = bigrams[bigram]\n",
    "            if count >= 6:\n",
    "                links[i] = 1\n",
    "\n",
    "        return links\n",
    "\n",
    "\n",
    "lf = CommonBigram()\n",
    "lf.apply(ncbi_docs)\n",
    "\n",
    "\n",
    "class ExtractedPhrase(LinkingRule):\n",
    "    def __init__(self, terms):\n",
    "        self.term_dict = {}\n",
    "\n",
    "        for term in terms:\n",
    "            term = [token.lower() for token in term]\n",
    "            if term[0] not in self.term_dict:\n",
    "                self.term_dict[term[0]] = []\n",
    "            self.term_dict[term[0]].append(term)\n",
    "\n",
    "        # Sorts the terms in decreasing order so that we match the longest\n",
    "        # first\n",
    "        for first_token in self.term_dict.keys():\n",
    "            to_sort = self.term_dict[first_token]\n",
    "            self.term_dict[first_token] = sorted(\n",
    "                to_sort, reverse=True, key=lambda x: len(x))\n",
    "\n",
    "    def apply_instance(self, instance):\n",
    "        tokens = [token.text.lower() for token in instance['tokens']]\n",
    "        links = [0] * len(instance['tokens'])\n",
    "\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            if tokens[i] in self.term_dict:\n",
    "                candidates = self.term_dict[tokens[i]]\n",
    "                for c in candidates:\n",
    "                    # Checks whether normalized AllenNLP tokens equal the list\n",
    "                    # of string tokens defining the term in the dictionary\n",
    "                    if i + len(c) <= len(tokens):\n",
    "                        equal = True\n",
    "                        for j in range(len(c)):\n",
    "                            if tokens[i + j] != c[j]:\n",
    "                                equal = False\n",
    "                                break\n",
    "\n",
    "                        # If tokens match, labels the instance tokens\n",
    "                        if equal:\n",
    "                            for j in range(i + 1, i + len(c)):\n",
    "                                links[j] = 1\n",
    "                            i = i + len(c) - 1\n",
    "                            break\n",
    "            i += 1\n",
    "\n",
    "        return links\n",
    "\n",
    "lf = ExtractedPhrase(dict_full)\n",
    "lf.apply(ncbi_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_to_token_span(tokens: List[str],\n",
    "                      text: str,\n",
    "                      txt_spans: List[tuple]):\n",
    "    \"\"\"\n",
    "    Transfer text-domain spans to token-domain spans\n",
    "    :param tokens: tokens\n",
    "    :param text: text\n",
    "    :param txt_spans: text spans tuples: (start, end, ...)\n",
    "    :return: a list of transferred span tuples.\n",
    "    \"\"\"\n",
    "    token_indices = get_original_spans(tokens, text)\n",
    "    tgt_spans = list()\n",
    "    for txt_span in txt_spans:\n",
    "        spacy_start = txt_span[0]\n",
    "        spacy_end = txt_span[1]\n",
    "        start = None\n",
    "        end = None\n",
    "        for i, (s, e) in enumerate(token_indices):\n",
    "            if s <= spacy_start < e:\n",
    "                start = i\n",
    "            if s <= spacy_end <= e:\n",
    "                end = i + 1\n",
    "            if (start is not None) and (end is not None):\n",
    "                break\n",
    "        assert (start is not None) and (end is not None), ValueError(\"input spans out of scope\")\n",
    "        tgt_spans.append((start, end))\n",
    "    return tgt_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respan(src_tokens: List[str],\n",
    "           tgt_tokens: List[str],\n",
    "           src_span: List[tuple]):\n",
    "    \"\"\"\n",
    "    transfer original spans to target spans\n",
    "    :param src_tokens: source tokens\n",
    "    :param tgt_tokens: target tokens\n",
    "    :param src_span: a list of span tuples. The first element in the tuple\n",
    "    should be the start index and the second should be the end index\n",
    "    :return: a list of transferred span tuples.\n",
    "    \"\"\"\n",
    "    s2t, _ = get_alignments(src_tokens, tgt_tokens)\n",
    "    tgt_spans = list()\n",
    "    for spans in src_span:\n",
    "        start = s2t[spans[0]][0]\n",
    "        if spans[1] < len(s2t):\n",
    "            end = s2t[spans[1]-1][-1] + 1\n",
    "        else:\n",
    "            end = s2t[-1][-1]\n",
    "        if end == start:\n",
    "            end += 1\n",
    "        tgt_spans.append((start, end))\n",
    "\n",
    "    return tgt_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_span(labels: List[str],\n",
    "                  scheme: Optional[str] = 'BIO') -> dict:\n",
    "    \"\"\"\n",
    "    convert labels to spans\n",
    "    :param labels: a list of labels\n",
    "    :param scheme: labeling scheme, in ['BIO', 'BILOU'].\n",
    "    :return: labeled spans, a list of tuples (start_idx, end_idx, label)\n",
    "    \"\"\"\n",
    "    assert scheme in ['BIO', 'BILOU'], ValueError(\"unknown labeling scheme\")\n",
    "\n",
    "    labeled_spans = dict()\n",
    "    i = 0\n",
    "    while i < len(labels):\n",
    "        if labels[i] == 'O' or labels[i] == 'ABS':\n",
    "            i += 1\n",
    "            continue\n",
    "        else:\n",
    "            if scheme == 'BIO':\n",
    "                if labels[i][0] == 'B':\n",
    "                    start = i\n",
    "                    lb = labels[i][2:]\n",
    "                    i += 1\n",
    "                    try:\n",
    "                        while labels[i][0] == 'I':\n",
    "                            i += 1\n",
    "                        end = i\n",
    "                        labeled_spans[(start, end)] = lb\n",
    "                    except IndexError:\n",
    "                        end = i\n",
    "                        labeled_spans[(start, end)] = lb\n",
    "                        i += 1\n",
    "                # this should not happen\n",
    "                elif labels[i][0] == 'I':\n",
    "                    i += 1\n",
    "            elif scheme == 'BILOU':\n",
    "                if labels[i][0] == 'U':\n",
    "                    start = i\n",
    "                    end = i + 1\n",
    "                    lb = labels[i][2:]\n",
    "                    labeled_spans[(start, end)] = lb\n",
    "                    i += 1\n",
    "                elif labels[i][0] == 'B':\n",
    "                    start = i\n",
    "                    lb = labels[i][2:]\n",
    "                    i += 1\n",
    "                    try:\n",
    "                        while labels[i][0] != 'L':\n",
    "                            i += 1\n",
    "                        end = i\n",
    "                        labeled_spans[(start, end)] = lb\n",
    "                    except IndexError:\n",
    "                        end = i\n",
    "                        labeled_spans[(start, end)] = lb\n",
    "                        break\n",
    "                    i += 1\n",
    "                else:\n",
    "                    i += 1\n",
    "\n",
    "    return labeled_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bert_emb(sents: List[str],\n",
    "                   tokenizer,\n",
    "                   model,\n",
    "                   device: str):\n",
    "    bert_embs = list()\n",
    "    for i, sent in enumerate(sents):\n",
    "\n",
    "        joint_sent = ' '.join(sent)\n",
    "        bert_tokens = tokenizer.tokenize(joint_sent)\n",
    "\n",
    "        input_ids = torch.tensor([tokenizer.encode(joint_sent, add_special_tokens=True)], device=device)\n",
    "        # calculate BERT last layer embeddings\n",
    "        with torch.no_grad():\n",
    "            last_hidden_states = model(input_ids)[0].squeeze(0).to('cpu')\n",
    "            trunc_hidden_states = last_hidden_states[1:-1, :]\n",
    "\n",
    "        ori2bert, bert2ori = get_alignments(sent, bert_tokens)\n",
    "\n",
    "        emb_list = list()\n",
    "        for idx in ori2bert:\n",
    "            emb = trunc_hidden_states[idx, :]\n",
    "            emb_list.append(emb.mean(dim=0))\n",
    "\n",
    "        # TODO: using the embedding of [CLS] may not be the best idea\n",
    "        # It does not matter since that embedding is not used in the training\n",
    "        emb_list = [last_hidden_states[0, :]] + emb_list\n",
    "        bert_emb = torch.stack(emb_list)\n",
    "        bert_embs.append(bert_emb)\n",
    "    return bert_embs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulate text, true labels and weak labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../data/NCBI/{file_name}', 'r') as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = list()\n",
    "clines = None\n",
    "for l in lines:\n",
    "    if l == '\\n':\n",
    "        if clines is not None:\n",
    "            clusters.append(clines)\n",
    "        clines = list()\n",
    "    else:\n",
    "        clines.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_token_list = list()\n",
    "src_anno_list = list()\n",
    "weak_anno_list = list()\n",
    "link_anno_list = list()\n",
    "\n",
    "allen_data = ncbi_docs\n",
    "mapping_dict = {0:'O', 1:'I'}\n",
    "\n",
    "for src, allen_annos in zip(clusters, allen_data):\n",
    "    \n",
    "    # handle the data read from the source text\n",
    "    src_txt = src[0].split('|')[2] + src[1].split('|')[2]\n",
    "    src_tokens = word_tokenize(src_txt)\n",
    "    for i in range(len(src_tokens)):\n",
    "        if src_tokens[i] == r'``' or src_tokens[i] == r\"''\":\n",
    "            src_tokens[i] = r'\"'\n",
    "    char_spans = list()\n",
    "    for annos in src[2:]:\n",
    "        anno_info = annos.strip().split('\\t')\n",
    "        start = int(anno_info[1])\n",
    "        end = int(anno_info[2])\n",
    "        char_spans.append((start, end))\n",
    "    src_spans = txt_to_token_span(src_tokens, src_txt, char_spans)\n",
    "    \n",
    "    src_annos = dict()\n",
    "    for span in src_spans:\n",
    "        src_annos[span] = LABEL\n",
    "\n",
    "    src_token_list.append(src_tokens)\n",
    "    src_anno_list.append(src_annos)\n",
    "    \n",
    "    # handle the data constructed using Allennlp\n",
    "    allen_tokens = list(map(str, allen_annos['tokens']))\n",
    "    weak_anno = dict()\n",
    "    \n",
    "    for k in allen_annos['WISER_LABELS']:\n",
    "        std_lbs = allen_annos['WISER_LABELS'][k][:]\n",
    "        \n",
    "        pre_anno = 'O'\n",
    "        for i in range(len(std_lbs)):\n",
    "            current_anno = std_lbs[i]\n",
    "            if std_lbs[i] == 'I':\n",
    "                if pre_anno != 'I':\n",
    "                    std_lbs[i] = 'B-' + LABEL\n",
    "                else:\n",
    "                    std_lbs[i] = 'I-' + LABEL\n",
    "            pre_anno = current_anno\n",
    "        weak_span = label_to_span(std_lbs)\n",
    "\n",
    "        src_weak_span = respan(allen_tokens, src_tokens, weak_span)\n",
    "        src_weak_anno = dict()\n",
    "        for span in src_weak_span:\n",
    "            src_weak_anno[span] = [(LABEL, 1.0)]\n",
    "            \n",
    "        weak_anno[k] = src_weak_anno\n",
    "    weak_anno_list.append(weak_anno)\n",
    "\n",
    "    \n",
    "    linked_dict = dict()\n",
    "    for src, entity_lbs in allen_annos['WISER_LINKS'].items():\n",
    "        entity_lbs = [mapping_dict[lb] for lb in entity_lbs]\n",
    "\n",
    "        pre_anno = 'O'\n",
    "        for i in range(len(entity_lbs)):\n",
    "            current_anno = entity_lbs[i]\n",
    "            if entity_lbs[i] == 'I':\n",
    "                if pre_anno != 'I':\n",
    "                    entity_lbs[i] = 'B-' + LINK\n",
    "                else:\n",
    "                    entity_lbs[i] = 'I-' + LINK\n",
    "            pre_anno = current_anno\n",
    "\n",
    "        entity_spans = label_to_span(entity_lbs)\n",
    "        complete_span = dict()\n",
    "        for (start, end), lb in entity_spans.items():\n",
    "            if start != 0:\n",
    "                start = start - 1\n",
    "            complete_span[(start, end)] = lb\n",
    "        src_link_span = respan(allen_tokens, src_tokens, complete_span)\n",
    "        linked_dict[src] = src_link_span\n",
    "    link_anno_list.append(linked_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_link_anno_list = list()\n",
    "for tag_anno, link_anno in zip(weak_anno_list, link_anno_list):\n",
    "    tag_spans = list()\n",
    "    for src, spans in tag_anno.items():\n",
    "        tag_spans += list(spans.keys())\n",
    "    for i in range(len(tag_spans)):\n",
    "        tag_spans[i] = set(range(tag_spans[i][0], tag_spans[i][1]))\n",
    "    \n",
    "    link_entities = dict()\n",
    "    for src, spans in link_anno.items():\n",
    "        valid_spans = list()\n",
    "        for span in spans:\n",
    "            if span[1] - span[0] == 1:\n",
    "                continue\n",
    "            span_set = set(range(span[0], span[1]))\n",
    "            for tag_span in tag_spans:\n",
    "                if span_set.intersection(tag_span):\n",
    "                    valid_spans.append(span)\n",
    "        valid_anno = {span: [(LABEL, 1.0)] for span in valid_spans}\n",
    "        link_entities[src] = valid_anno\n",
    "    updated_link_anno_list.append(link_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_anno_list = list()\n",
    "for tag_anno, link_anno in zip(weak_anno_list, updated_link_anno_list):\n",
    "    comb_anno = dict()\n",
    "    for k, v in tag_anno.items():\n",
    "        comb_anno[k] = v\n",
    "    for k, v in link_anno.items():\n",
    "        comb_anno[k] = v\n",
    "    combined_anno_list.append(comb_anno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Bert Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n",
    "\n",
    "model = AutoModel.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "standarized_sents = list()\n",
    "o2n_map = list()\n",
    "n=0\n",
    "for i, sents in enumerate(src_token_list):\n",
    "    joint_sent = ' '.join(sents)\n",
    "    len_bert_tokens = len(tokenizer.tokenize(joint_sent))\n",
    "    if len_bert_tokens >= 510:        \n",
    "        sts = sent_tokenize(joint_sent)\n",
    "        \n",
    "        sent_lens = list()\n",
    "        for st in sts:\n",
    "            sent_lens.append(len(word_tokenize(st)))\n",
    "        ends = [np.sum(sent_lens[:i]) for i in range(1, len(sent_lens)+1)]\n",
    "        \n",
    "        nearest_end_idx1 = np.argmin((np.array(ends) - len_bert_tokens / 3) ** 2)\n",
    "        nearest_end_idx2 = np.argmin((np.array(ends) - len_bert_tokens / 3 * 2) ** 2)\n",
    "        split_1 = sents[:ends[nearest_end_idx1]]\n",
    "        split_2 = sents[ends[nearest_end_idx1]:ends[nearest_end_idx2]]\n",
    "        split_3 = sents[ends[nearest_end_idx2]:]\n",
    "        standarized_sents.append(split_1)\n",
    "        standarized_sents.append(split_2)\n",
    "        standarized_sents.append(split_3)\n",
    "        o2n_map.append([n, n+1, n+2])\n",
    "        n += 3\n",
    "\n",
    "    else:\n",
    "        standarized_sents.append(sents)\n",
    "        o2n_map.append([n])\n",
    "        n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sents in enumerate(standarized_sents):\n",
    "    joint_sent = ' '.join(sents)\n",
    "    if len(tokenizer.tokenize(joint_sent)) >= 510:\n",
    "        print(i, len(sents), len(tokenizer.tokenize(joint_sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# device=torch.device('cpu')\n",
    "model = model.to(device)\n",
    "embs = build_bert_emb(standarized_sents, tokenizer, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_embs = list()\n",
    "for o2n in o2n_map:\n",
    "    if len(o2n) == 1:\n",
    "        combined_embs.append(embs[o2n[0]])\n",
    "    else:\n",
    "        cat_emb = torch.cat([embs[o2n[0]], embs[o2n[1]][1:], embs[o2n[2]][1:]], dim=0)\n",
    "        combined_embs.append(cat_emb)\n",
    "for emb, sent in zip(combined_embs, src_token_list):\n",
    "    assert len(emb) == len(sent) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"sentences\": src_token_list,\n",
    "    \"annotations\": combined_anno_list,\n",
    "    \"labels\": src_anno_list\n",
    "}\n",
    "\n",
    "torch.save(data, f\"NCBI-linked-{DATA_PARTITION}.pt\")\n",
    "torch.save(combined_embs, f\"NCBI-emb-{DATA_PARTITION}.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
