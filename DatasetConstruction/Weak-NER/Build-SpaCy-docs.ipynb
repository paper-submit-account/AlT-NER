{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from tokenizations import get_alignments, get_original_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        labels = json.load(f)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def load_conll_2003_data(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    app_sentence = list()\n",
    "    all_sentence = list()\n",
    "    sentence = list()\n",
    "    all_labels = list()\n",
    "    labels = list()\n",
    "    for line in lines:\n",
    "        try:\n",
    "            token, _, _, ner_label = line.strip().split()\n",
    "            sentence.append(token)\n",
    "            labels.append(ner_label)\n",
    "        except ValueError:\n",
    "            app_sentence.append(sentence + [\"@SB@\"])\n",
    "            all_sentence.append(sentence)\n",
    "            all_labels.append(labels)\n",
    "            sentence = list()\n",
    "            labels = list()\n",
    "\n",
    "    for sentence, labels in zip(all_sentence, all_labels):\n",
    "        assert len(sentence) == len(labels)\n",
    "\n",
    "    return app_sentence, all_sentence, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r'data/conll2003/'\n",
    "data_name = 'dev.txt'\n",
    "app_tokens, all_tokens, all_labels = load_conll_2003_data(os.path.join(data_dir, data_name))\n",
    "idx2label = load_labels(os.path.join('data', 'CoNLL2003-labels.json'))\n",
    "label2idx = {v: k for k, v in enumerate(idx2label)}\n",
    "\n",
    "for labels in all_labels:\n",
    "    for lb in labels:\n",
    "        if lb not in idx2label:\n",
    "            raise ValueError\n",
    "\n",
    "lb_indices = [[0] + [label2idx[lb] for lb in lbs] for lbs in all_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list()\n",
    "for tokens in all_tokens:\n",
    "    sentences.append(' '.join(tokens))\n",
    "app_sentences = list()\n",
    "for tokens in app_tokens:\n",
    "    app_sentences.append(' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = ' '.join(sentences[:10])\n",
    "app_sents = ' '.join(app_sentences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "app_doc = nlp(app_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_indices = list()\n",
    "start_indices = list()\n",
    "for i, token in enumerate(doc):\n",
    "    if token.text == '@SB@':\n",
    "        sep_indices.append(i)\n",
    "        start_indices.append(i+1 - len(sep_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_custom_boundaries(doc):\n",
    "#     for token in doc[:-1]:\n",
    "#         if token.text == '@SB@':\n",
    "#             doc[token.i+1].is_sent_start = True\n",
    "    for i in range(len(doc)):\n",
    "        doc[i].is_sent_start=False\n",
    "    doc[0].is_sent_start=True\n",
    "    for i in start_indices[:-1]:\n",
    "        doc[i].is_sent_start=True\n",
    "    return doc\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(set_custom_boundaries, before=\"parser\")\n",
    "doc = nlp(sents)\n",
    "for sent in doc.sents:\n",
    "    for t in sent:\n",
    "        print(t, end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r'data/conll2003/'\n",
    "data_name_ori = 'dev.txt'\n",
    "data_name_new = 'eng.testa'\n",
    "\n",
    "with open(os.path.join(data_dir, data_name_ori), 'r') as f:\n",
    "    corpus1 = f.readlines()\n",
    "with open(os.path.join(data_dir, data_name_new), 'r') as f:\n",
    "    corpus2 = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "j = 0\n",
    "corpus_sent = list()\n",
    "sep_corpus = list()\n",
    "sep_sentences = list()\n",
    "sentences = list()\n",
    "sentence = list()\n",
    "corpus_label = list()\n",
    "label_seqs = list()\n",
    "label_seq = list()\n",
    "while i<len(corpus1) and j<len(corpus2):\n",
    "    try:\n",
    "        token2, _, _, ner_label2 = corpus2[j].strip().split()\n",
    "        if token2 == '-DOCSTART-':\n",
    "            if sentences:\n",
    "                corpus_sent.append(sentences)\n",
    "                sep_corpus.append(sep_sentences)\n",
    "                corpus_label.append(label_seqs)\n",
    "                sentences = list()\n",
    "                sep_sentences = list()\n",
    "                label_seqs = list()\n",
    "            j += 2  # skip a new line\n",
    "        else:\n",
    "            _, _, _, ner_label1 = corpus1[i].strip().split()\n",
    "            sentence.append(token2)\n",
    "            label_seq.append(ner_label1)\n",
    "            i += 1\n",
    "            j += 1\n",
    "        \n",
    "    except ValueError:\n",
    "        if sentence:\n",
    "            sep_sentences.append(sentence + [\"@SB@\"])\n",
    "            sentences.append(sentence)\n",
    "            label_seqs.append(label_seq)\n",
    "            sentence = list()\n",
    "            label_seq = list()\n",
    "    \n",
    "        i += 1\n",
    "        j += 1\n",
    "\n",
    "corpus_sent.append(sentences)\n",
    "sep_corpus.append(sep_sentences)\n",
    "corpus_label.append(label_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"documents\": corpus_sent,\n",
    "    \"labels\": corpus_label\n",
    "}\n",
    "torch.save(data, \"CoNLL03-dev.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_custom_boundaries(doc, start_indices):\n",
    "    for i in range(len(doc)):\n",
    "        doc[i].is_sent_start=False\n",
    "    doc[0].is_sent_start=True\n",
    "    for i in start_indices[:-1]:\n",
    "        doc[i].is_sent_start=True\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for cp, sep_cp in zip(corpus_sent, sep_corpus):\n",
    "    sentences = list()\n",
    "    for tokens in cp:\n",
    "        sentences.append(' '.join(tokens))\n",
    "    sep_sentences = list()\n",
    "    for tokens in sep_cp:\n",
    "        sep_sentences.append(' '.join(tokens))\n",
    "\n",
    "    sents = ' '.join(sentences)\n",
    "    sep_sents = ' '.join(sep_sentences)\n",
    "    \n",
    "    sep_doc = sep_nlp(sep_sents)\n",
    "    n_sep = 0\n",
    "    start_indices = list()\n",
    "    for i, token in enumerate(sep_doc):\n",
    "        if token.text == '@SB@':\n",
    "            n_sep += 1\n",
    "            start_indices.append(i+1 - n_sep)\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    nlp.add_pipe(lambda doc: set_custom_boundaries(doc, start_indices=start_indices), before=\"parser\")\n",
    "    doc = nlp(sents)\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(docs, 'something.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
