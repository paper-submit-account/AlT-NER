{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wiser.data.dataset_readers.cdr import CDRCombinedDatasetReader\n",
    "from wiser.rules import TaggingRule, LinkingRule, DictionaryMatcher\n",
    "from wiser.generative import get_label_to_ix, get_rules\n",
    "from labelmodels import *\n",
    "from wiser.generative import train_generative_model\n",
    "from labelmodels import LearningConfig\n",
    "from wiser.generative import evaluate_generative_model\n",
    "from wiser.data import save_label_distribution\n",
    "from wiser.eval import *\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tokenizations import get_alignments, get_original_spans\n",
    "from typing import List, Optional, Tuple\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from xml.etree import ElementTree\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc63ae1188074c0fa8490e0f7efda7d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='reading instances'), FloatProgress(value=1.0, bar_style='info', layout=Layout(width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0db3bb6c5fad4d5a8c5272d754efa586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd02903f702b4f13b1c467feef92cd18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='reading instances'), FloatProgress(value=1.0, bar_style='info', layout=Layout(width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76fc7f837956460a9846e36a6e13c3f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae5d1a7b5a754996a2488e5ddd7899ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='reading instances'), FloatProgress(value=1.0, bar_style='info', layout=Layout(width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6942fd5424542869492479fd4b088b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cdr_reader = CDRCombinedDatasetReader()\n",
    "train_data = cdr_reader.read('../data/BC5CDR/CDR_TrainingSet.BioC.xml')\n",
    "dev_data = cdr_reader.read('../data/BC5CDR/CDR_DevelopmentSet.BioC.xml')\n",
    "test_data = cdr_reader.read('../data/BC5CDR/CDR_TestSet.BioC.xml')\n",
    "\n",
    "cdr_docs = train_data + dev_data + test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57785b013bd14710a0d879e499b8db87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aaa3b03da95443f84f33cb64898e529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be6e874a38d14231847b5749fa09155c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc678bc74878446093e008b2b3261fcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd4c2908e6249c8acecdefe9c51726a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "670db15b339342fd864d0c9faa119084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d9ac32d411a4732988f9ea9c530da4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab905391302e474eb3d541023e7116eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cba08b4c9e3942d48f899db681efc6dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "118b42ca61934042a900dfb64c854e99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "317e2aeb37ea4d43b6bec392c2239db3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9085053dac94f84a8fe88d65af5500d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f68e80ab684d1a87c2f916ad12e7c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc0624bda9434172b946397e6c2f3fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "971bde4ae5e5465ba99b8462de21c258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5cb77ff492740649ece28418300b09d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71e32e05192242fe9cdded005c4faba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "763e50fe317c468584514a60509dfb93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c0d172e677243fea1f1097e8ab1bdfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38fe386c9431426a8898c6acc52980f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b6e88fdce1a4b77a94e0f6968f39ff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b55e9545b5e4f75a327dab814c1d38b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ebec2ad94cb4db4b140068ef9373cd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42e06b6163c249c3a82c8f5a4db98283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20570b775f8745f59fe65d7498cbef47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3640be46fcd444c7a2f7ef19675e658a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d34f74b5b04214b02769177b099571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dict_core_chem = set()\n",
    "dict_core_chem_exact = set()\n",
    "dict_core_dis = set()\n",
    "dict_core_dis_exact = set()\n",
    "\n",
    "with open('../data/AutoNER_dicts/BC5CDR/dict_core.txt') as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip().split(None, 1)\n",
    "        entity_type = line[0]\n",
    "        tokens = cdr_reader.get_tokenizer()(line[1])\n",
    "        term = tuple([str(x) for x in tokens])\n",
    "\n",
    "        if len(term) > 1 or len(term[0]) > 3:\n",
    "            if entity_type == 'Chemical':\n",
    "                dict_core_chem.add(term)\n",
    "            elif entity_type == 'Disease':\n",
    "                dict_core_dis.add(term)\n",
    "            else:\n",
    "                raise Exception()\n",
    "        else:\n",
    "            if entity_type == 'Chemical':\n",
    "                dict_core_chem_exact.add(term)\n",
    "            elif entity_type == 'Disease':\n",
    "                dict_core_dis_exact.add(term)\n",
    "            else:\n",
    "                raise Exception()\n",
    "\n",
    "lf = DictionaryMatcher(\n",
    "    \"DictCore-Chemical\",\n",
    "    dict_core_chem,\n",
    "    i_label=\"I-Chemical\",\n",
    "    uncased=True)\n",
    "lf.apply(cdr_docs)\n",
    "lf = DictionaryMatcher(\n",
    "    \"DictCore-Chemical-Exact\",\n",
    "    dict_core_chem_exact,\n",
    "    i_label=\"I-Chemical\",\n",
    "    uncased=False)\n",
    "lf.apply(cdr_docs)\n",
    "lf = DictionaryMatcher(\n",
    "    \"DictCore-Disease\",\n",
    "    dict_core_dis,\n",
    "    i_label=\"I-Disease\",\n",
    "    uncased=True)\n",
    "lf.apply(cdr_docs)\n",
    "lf = DictionaryMatcher(\n",
    "    \"DictCore-Disease-Exact\",\n",
    "    dict_core_dis_exact,\n",
    "    i_label=\"I-Disease\",\n",
    "    uncased=False)\n",
    "lf.apply(cdr_docs)\n",
    "\n",
    "terms = []\n",
    "with open('../data/umls/umls_element_ion_or_isotope.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        terms.append(line.strip().split(\" \"))\n",
    "lf = DictionaryMatcher(\n",
    "    \"Element, Ion, or Isotope\",\n",
    "    terms,\n",
    "    i_label='I-Chemical',\n",
    "    uncased=True,\n",
    "    match_lemmas=True)\n",
    "lf.apply(cdr_docs)\n",
    "\n",
    "\n",
    "terms = []\n",
    "with open('../data/umls/umls_organic_chemical.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        terms.append(line.strip().split(\" \"))\n",
    "lf = DictionaryMatcher(\n",
    "    \"Organic Chemical\",\n",
    "    terms,\n",
    "    i_label='I-Chemical',\n",
    "    uncased=True,\n",
    "    match_lemmas=True)\n",
    "lf.apply(cdr_docs)\n",
    "\n",
    "\n",
    "terms = []\n",
    "with open('../data/umls/umls_antibiotic.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        terms.append(line.strip().split(\" \"))\n",
    "lf = DictionaryMatcher(\n",
    "    \"Antibiotic\",\n",
    "    terms,\n",
    "    i_label='I-Chemical',\n",
    "    uncased=True,\n",
    "    match_lemmas=True)\n",
    "lf.apply(cdr_docs)\n",
    "\n",
    "\n",
    "terms = []\n",
    "with open('../data/umls/umls_disease_or_syndrome.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        terms.append(line.strip().split(\" \"))\n",
    "lf = DictionaryMatcher(\n",
    "    \"Disease or Syndrome\",\n",
    "    terms,\n",
    "    i_label='I-Disease',\n",
    "    uncased=True,\n",
    "    match_lemmas=True)\n",
    "lf.apply(cdr_docs)\n",
    "\n",
    "terms = []\n",
    "with open('../data/umls/umls_body_part.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        terms.append(line.strip().split(\" \"))\n",
    "lf = DictionaryMatcher(\n",
    "    \"TEMP\",\n",
    "    terms,\n",
    "    i_label='TEMP',\n",
    "    uncased=True,\n",
    "    match_lemmas=True)\n",
    "lf.apply(cdr_docs)\n",
    "\n",
    "\n",
    "class BodyTerms(TaggingRule):\n",
    "    def apply_instance(self, instance):\n",
    "        tokens = [token.text.lower() for token in instance['tokens']]\n",
    "        labels = ['ABS'] * len(tokens)\n",
    "\n",
    "        terms = {\"cancer\", \"cancers\", \"damage\", \"disease\", \"diseases\", \"pain\", \"injury\", \"injuries\"}\n",
    "\n",
    "        for i in range(0, len(tokens) - 1):\n",
    "            if instance['WISER_LABELS']['TEMP'][i] == 'TEMP':\n",
    "                if tokens[i + 1] in terms:\n",
    "                    labels[i] = \"I-Disease\"\n",
    "                    labels[i + 1] = \"I-Disease\"\n",
    "        return labels\n",
    "\n",
    "\n",
    "lf = BodyTerms()\n",
    "lf.apply(cdr_docs)\n",
    "\n",
    "for doc in cdr_docs:\n",
    "    del doc['WISER_LABELS']['TEMP']\n",
    "\n",
    "\n",
    "class Acronyms(TaggingRule):\n",
    "    other_lfs = {\n",
    "        'I-Chemical': (\"Antibiotic\", \"Element, Ion, or Isotope\", \"Organic Chemical\"),\n",
    "        'I-Disease': (\"BodyTerms\", \"Disease or Syndrome\")\n",
    "    }\n",
    "\n",
    "    def apply_instance(self, instance):\n",
    "        labels = ['ABS'] * len(instance['tokens'])\n",
    "\n",
    "        active = False\n",
    "        for tag, lf_names in self.other_lfs.items():\n",
    "            acronyms = set()\n",
    "            for lf_name in lf_names:\n",
    "                for i in range(len(instance['tokens']) - 2):\n",
    "                    if instance['WISER_LABELS'][lf_name][i] == tag:\n",
    "                        active = True\n",
    "                    elif active and instance['tokens'][i].text == '(' and instance['tokens'][i + 2].pos_ == \"PUNCT\" and instance['tokens'][i + 1].pos_ != \"NUM\":\n",
    "                        acronyms.add(instance['tokens'][i + 1].text)\n",
    "                        active = False\n",
    "                    else:\n",
    "                        active = False\n",
    "\n",
    "            for i, token in enumerate(instance['tokens']):\n",
    "                if token.text in acronyms:\n",
    "                    labels[i] = tag\n",
    "\n",
    "        return labels\n",
    "\n",
    "\n",
    "lf = Acronyms()\n",
    "lf.apply(cdr_docs)\n",
    "\n",
    "\n",
    "class Damage(TaggingRule):\n",
    "\n",
    "    def apply_instance(self, instance):\n",
    "        labels = ['ABS'] * len(instance['tokens'])\n",
    "\n",
    "        for i in range(len(instance['tokens']) - 1):\n",
    "            if instance['tokens'][i].dep_ == 'compound' and instance['tokens'][i +\n",
    "                                                                               1].lemma_ == 'damage':\n",
    "                labels[i] = 'I-Disease'\n",
    "                labels[i + 1] = 'I-Disease'\n",
    "\n",
    "                # Adds any other compound tokens before the phrase\n",
    "                for j in range(i - 1, -1, -1):\n",
    "                    if instance['tokens'][j].dep_ == 'compound':\n",
    "                        labels[j] = 'I-Disease'\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "        return labels\n",
    "\n",
    "\n",
    "lf = Damage()\n",
    "lf.apply(cdr_docs)\n",
    "\n",
    "\n",
    "class Disease(TaggingRule):\n",
    "\n",
    "    def apply_instance(self, instance):\n",
    "        labels = ['ABS'] * len(instance['tokens'])\n",
    "\n",
    "        for i in range(len(instance['tokens']) - 1):\n",
    "            if instance['tokens'][i].dep_ == 'compound' and instance['tokens'][i +\n",
    "                                                                               1].lemma_ == 'disease':\n",
    "                labels[i] = 'I-Disease'\n",
    "                labels[i + 1] = 'I-Disease'\n",
    "\n",
    "                # Adds any other compound tokens before the phrase\n",
    "                for j in range(i - 1, -1, -1):\n",
    "                    if instance['tokens'][j].dep_ == 'compound':\n",
    "                        labels[j] = 'I-Disease'\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "        return labels\n",
    "\n",
    "\n",
    "lf = Disease()\n",
    "lf.apply(cdr_docs)\n",
    "\n",
    "\n",
    "class Disorder(TaggingRule):\n",
    "\n",
    "    def apply_instance(self, instance):\n",
    "        labels = ['ABS'] * len(instance['tokens'])\n",
    "\n",
    "        for i in range(len(instance['tokens']) - 1):\n",
    "            if instance['tokens'][i].dep_ == 'compound' and instance['tokens'][i +\n",
    "                                                                               1].lemma_ == 'disorder':\n",
    "                labels[i] = 'I-Disease'\n",
    "                labels[i + 1] = 'I-Disease'\n",
    "\n",
    "                # Adds any other compound tokens before the phrase\n",
    "                for j in range(i - 1, -1, -1):\n",
    "                    if instance['tokens'][j].dep_ == 'compound':\n",
    "                        labels[j] = 'I-Disease'\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "        return labels\n",
    "\n",
    "\n",
    "lf = Disorder()\n",
    "lf.apply(cdr_docs)\n",
    "\n",
    "\n",
    "class Lesion(TaggingRule):\n",
    "\n",
    "    def apply_instance(self, instance):\n",
    "        labels = ['ABS'] * len(instance['tokens'])\n",
    "\n",
    "        for i in range(len(instance['tokens']) - 1):\n",
    "            if instance['tokens'][i].dep_ == 'compound' and instance['tokens'][i +\n",
    "                                                                               1].lemma_ == 'lesion':\n",
    "                labels[i] = 'I-Disease'\n",
    "                labels[i + 1] = 'I-Disease'\n",
    "\n",
    "                # Adds any other compound tokens before the phrase\n",
    "                for j in range(i - 1, -1, -1):\n",
    "                    if instance['tokens'][j].dep_ == 'compound':\n",
    "                        labels[j] = 'I-Disease'\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "        return labels\n",
    "\n",
    "\n",
    "lf = Lesion()\n",
    "lf.apply(cdr_docs)\n",
    "\n",
    "\n",
    "class Syndrome(TaggingRule):\n",
    "\n",
    "    def apply_instance(self, instance):\n",
    "        labels = ['ABS'] * len(instance['tokens'])\n",
    "\n",
    "        for i in range(len(instance['tokens']) - 1):\n",
    "            if instance['tokens'][i].dep_ == 'compound' and instance['tokens'][i +\n",
    "                                                                               1].lemma_ == 'syndrome':\n",
    "                labels[i] = 'I-Disease'\n",
    "                labels[i + 1] = 'I-Disease'\n",
    "\n",
    "                # Adds any other compound tokens before the phrase\n",
    "                for j in range(i - 1, -1, -1):\n",
    "                    if instance['tokens'][j].dep_ == 'compound':\n",
    "                        labels[j] = 'I-Disease'\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "        return labels\n",
    "\n",
    "\n",
    "lf = Syndrome()\n",
    "lf.apply(cdr_docs)\n",
    "\n",
    "\n",
    "exceptions = {'determine', 'baseline', 'decline',\n",
    "              'examine', 'pontine', 'vaccine',\n",
    "              'routine', 'crystalline', 'migraine',\n",
    "              'alkaline', 'midline', 'borderline',\n",
    "              'cocaine', 'medicine', 'medline',\n",
    "              'asystole', 'control', 'protocol',\n",
    "              'alcohol', 'aerosol', 'peptide',\n",
    "              'provide', 'outside', 'intestine',\n",
    "              'combine', 'delirium', 'VIP'}\n",
    "\n",
    "suffixes = ('ine', 'ole', 'ol', 'ide', 'ine', 'ium', 'epam')\n",
    "\n",
    "\n",
    "class ChemicalSuffixes(TaggingRule):\n",
    "    def apply_instance(self, instance):\n",
    "\n",
    "        labels = ['ABS'] * len(instance['tokens'])\n",
    "\n",
    "        acronyms = set()\n",
    "        for i, t in enumerate(instance['tokens']):\n",
    "            if len(t.lemma_) >= 7 and t.lemma_ not in exceptions and t.lemma_.endswith(\n",
    "                    suffixes):\n",
    "                labels[i] = 'I-Chemical'\n",
    "\n",
    "                if i < len(instance['tokens']) - 3 and instance['tokens'][i + \\\n",
    "                           1].text == '(' and instance['tokens'][i + 3].text == ')':\n",
    "                    acronyms.add(instance['tokens'][i + 2].text)\n",
    "\n",
    "        for i, t in enumerate(instance['tokens']):\n",
    "            if t.text in acronyms and t.text not in exceptions:\n",
    "                labels[i] = 'I-Chemical'\n",
    "        return labels\n",
    "\n",
    "\n",
    "lf = ChemicalSuffixes()\n",
    "lf.apply(cdr_docs)\n",
    "\n",
    "\n",
    "class CancerLike(TaggingRule):\n",
    "    def apply_instance(self, instance):\n",
    "        tokens = [token.text.lower() for token in instance['tokens']]\n",
    "        labels = ['ABS'] * len(tokens)\n",
    "\n",
    "        suffixes = (\"edema\", \"toma\", \"coma\", \"noma\")\n",
    "\n",
    "        for i, token in enumerate(tokens):\n",
    "            for suffix in suffixes:\n",
    "                if token.endswith(suffix) or token.endswith(suffix + \"s\"):\n",
    "                    labels[i] = 'I-Disease'\n",
    "        return labels\n",
    "\n",
    "\n",
    "lf = CancerLike()\n",
    "lf.apply(cdr_docs)\n",
    "\n",
    "\n",
    "exceptions = {'diagnosis', 'apoptosis', 'prognosis', 'metabolism'}\n",
    "\n",
    "suffixes = (\"agia\", \"cardia\", \"trophy\", \"itis\",\n",
    "            \"emia\", \"enia\", \"pathy\", \"plasia\", \"lism\", \"osis\")\n",
    "\n",
    "\n",
    "class DiseaseSuffixes(TaggingRule):\n",
    "    def apply_instance(self, instance):\n",
    "        labels = ['ABS'] * len(instance['tokens'])\n",
    "\n",
    "        for i, t in enumerate(instance['tokens']):\n",
    "            if len(t.lemma_) >= 5 and t.lemma_.lower(\n",
    "            ) not in exceptions and t.lemma_.endswith(suffixes):\n",
    "                labels[i] = 'I-Disease'\n",
    "\n",
    "        return labels\n",
    "\n",
    "\n",
    "lf = DiseaseSuffixes()\n",
    "lf.apply(cdr_docs)\n",
    "\n",
    "\n",
    "exceptions = {'hypothesis', 'hypothesize', 'hypobaric', 'hyperbaric'}\n",
    "\n",
    "prefixes = ('hyper', 'hypo')\n",
    "\n",
    "\n",
    "class DiseasePrefixes(TaggingRule):\n",
    "    def apply_instance(self, instance):\n",
    "        labels = ['ABS'] * len(instance['tokens'])\n",
    "\n",
    "        for i, t in enumerate(instance['tokens']):\n",
    "            if len(t.lemma_) >= 5 and t.lemma_.lower(\n",
    "            ) not in exceptions and t.lemma_.startswith(prefixes):\n",
    "                if instance['tokens'][i].pos_ == \"NOUN\":\n",
    "                    labels[i] = 'I-Disease'\n",
    "\n",
    "        return labels\n",
    "\n",
    "\n",
    "lf = DiseasePrefixes()\n",
    "lf.apply(cdr_docs)\n",
    "\n",
    "\n",
    "exceptions = {\n",
    "    \"drug\",\n",
    "    \"pre\",\n",
    "    \"therapy\",\n",
    "    \"anesthetia\",\n",
    "    \"anesthetic\",\n",
    "    \"neuroleptic\",\n",
    "    \"saline\",\n",
    "    \"stimulus\"}\n",
    "\n",
    "\n",
    "class Induced(TaggingRule):\n",
    "    def apply_instance(self, instance):\n",
    "\n",
    "        labels = ['ABS'] * len(instance['tokens'])\n",
    "\n",
    "        for i in range(1, len(instance['tokens']) - 3):\n",
    "            lemma = instance['tokens'][i].lemma_.lower()\n",
    "            if instance['tokens'][i].text == '-' and instance['tokens'][i +\n",
    "                                                                        1].lemma_ == 'induce':\n",
    "                labels[i] = 'O'\n",
    "                labels[i + 1] = 'O'\n",
    "                if instance['tokens'][i -\n",
    "                                      1].lemma_ in exceptions or instance['tokens'][i -\n",
    "                                                                                    1].pos_ == \"PUNCT\":\n",
    "                    labels[i - 1] = 'O'\n",
    "                else:\n",
    "                    labels[i - 1] = 'I-Chemical'\n",
    "        return labels\n",
    "\n",
    "\n",
    "lf = Induced()\n",
    "lf.apply(cdr_docs)\n",
    "\n",
    "\n",
    "class Vitamin(TaggingRule):\n",
    "    def apply_instance(self, instance):\n",
    "\n",
    "        labels = ['ABS'] * len(instance['tokens'])\n",
    "\n",
    "        for i in range(len(instance['tokens']) - 1):\n",
    "            text = instance['tokens'][i].text.lower()\n",
    "            if instance['tokens'][i].text.lower() == 'vitamin':\n",
    "                labels[i] = 'I-Chemical'\n",
    "                if len(instance['tokens'][i +\n",
    "                                          1].text) <= 2 and instance['tokens'][i +\n",
    "                                                                               1].text.isupper():\n",
    "                    labels[i + 1] = 'I-Chemical'\n",
    "\n",
    "        return labels\n",
    "\n",
    "\n",
    "lf = Vitamin()\n",
    "lf.apply(cdr_docs)\n",
    "\n",
    "\n",
    "class Acid(TaggingRule):\n",
    "    def apply_instance(self, instance):\n",
    "\n",
    "        labels = ['ABS'] * len(instance['tokens'])\n",
    "\n",
    "        tokens = instance['tokens']\n",
    "\n",
    "        for i, t in enumerate(tokens):\n",
    "            if i > 0 and t.text.lower(\n",
    "            ) == 'acid' and tokens[i - 1].text.endswith('ic'):\n",
    "                labels[i] = 'I-Chemical'\n",
    "                labels[i - 1] = 'I-Chemical'\n",
    "\n",
    "        return labels\n",
    "\n",
    "\n",
    "lf = Acid()\n",
    "lf.apply(cdr_docs)\n",
    "\n",
    "\n",
    "class OtherPOS(TaggingRule):\n",
    "    other_pos = {\"ADP\", \"ADV\", \"DET\", \"VERB\"}\n",
    "\n",
    "    def apply_instance(self, instance):\n",
    "        labels = ['ABS'] * len(instance['tokens'])\n",
    "\n",
    "        for i in range(0, len(instance['tokens'])):\n",
    "            # Some chemicals with long names get tagged as verbs\n",
    "            if instance['tokens'][i].pos_ in self.other_pos and instance['WISER_LABELS'][\n",
    "                    'Organic Chemical'][i] == 'ABS' and instance['WISER_LABELS']['DictCore-Chemical'][i] == 'ABS':\n",
    "                labels[i] = \"O\"\n",
    "        return labels\n",
    "\n",
    "\n",
    "lf = OtherPOS()\n",
    "lf.apply(cdr_docs)\n",
    "\n",
    "\n",
    "stop_words = {\"a\", \"an\", \"as\", \"be\", \"but\", \"do\", \"even\",\n",
    "              \"for\", \"from\",\n",
    "              \"had\", \"has\", \"have\", \"i\", \"in\", \"is\", \"its\", \"just\",\n",
    "              \"may\", \"my\", \"no\", \"not\", \"on\", \"or\",\n",
    "              \"than\", \"that\", \"the\", \"these\", \"this\", \"those\", \"to\", \"very\",\n",
    "              \"what\", \"which\", \"who\", \"with\"}\n",
    "\n",
    "\n",
    "class StopWords(TaggingRule):\n",
    "\n",
    "    def apply_instance(self, instance):\n",
    "        labels = ['ABS'] * len(instance['tokens'])\n",
    "\n",
    "        for i in range(len(instance['tokens'])):\n",
    "            if instance['tokens'][i].lemma_ in stop_words:\n",
    "                labels[i] = 'O'\n",
    "        return labels\n",
    "\n",
    "\n",
    "lf = StopWords()\n",
    "lf.apply(cdr_docs)\n",
    "\n",
    "\n",
    "class CommonOther(TaggingRule):\n",
    "    other_lemmas = {'patient', '-PRON-', 'induce', 'after', 'study',\n",
    "                    'rat', 'mg', 'use', 'treatment', 'increase',\n",
    "                    'day', 'group', 'dose', 'treat', 'case', 'result',\n",
    "                    'kg', 'control', 'report', 'administration', 'follow',\n",
    "                    'level', 'suggest', 'develop', 'week', 'compare',\n",
    "                    'significantly', 'receive', 'mouse',\n",
    "                    'protein', 'infusion', 'output', 'area', 'effect',\n",
    "                    'rate', 'weight', 'size', 'time', 'year',\n",
    "                    'clinical', 'conclusion', 'outcome', 'man', 'woman',\n",
    "                    'model', 'concentration'}\n",
    "\n",
    "    def apply_instance(self, instance):\n",
    "        labels = ['ABS'] * len(instance['tokens'])\n",
    "        for i in range(len(instance['tokens'])):\n",
    "            if instance['tokens'][i].lemma_ in self.other_lemmas:\n",
    "                labels[i] = 'O'\n",
    "        return labels\n",
    "\n",
    "\n",
    "lf = CommonOther()\n",
    "lf.apply(cdr_docs)\n",
    "\n",
    "\n",
    "class Punctuation(TaggingRule):\n",
    "\n",
    "    other_punc = {\"?\", \"!\", \";\", \":\", \".\", \",\",\n",
    "                  \"%\", \"<\", \">\", \"=\", \"\\\\\"}\n",
    "\n",
    "    def apply_instance(self, instance):\n",
    "        labels = ['ABS'] * len(instance['tokens'])\n",
    "\n",
    "        for i in range(len(instance['tokens'])):\n",
    "            if instance['tokens'][i].text in self.other_punc:\n",
    "                labels[i] = 'O'\n",
    "        return labels\n",
    "\n",
    "\n",
    "lf = Punctuation()\n",
    "lf.apply(cdr_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linking Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb21e21abb46407a90c8f849bc9e4a63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c97e24a45abd4874a14b92bee52ea099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "717cd628366c46ba90b85921fbca069f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3c88d70a34c40be9393ee37fb9f3c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class PossessivePhrase(LinkingRule):\n",
    "    def apply_instance(self, instance):\n",
    "        links = [0] * len(instance['tokens'])\n",
    "        for i in range(1, len(instance['tokens'])):\n",
    "            if instance['tokens'][i -\n",
    "                                  1].text == \"'s\" or instance['tokens'][i].text == \"'s\":\n",
    "                links[i] = 1\n",
    "\n",
    "        return links\n",
    "\n",
    "\n",
    "lf = PossessivePhrase()\n",
    "lf.apply(cdr_docs)\n",
    "\n",
    "\n",
    "class HyphenatedPrefix(LinkingRule):\n",
    "    chem_mods = set([\"alpha\", \"beta\", \"gamma\", \"delta\", \"epsilon\"])\n",
    "\n",
    "    def apply_instance(self, instance):\n",
    "        links = [0] * len(instance['tokens'])\n",
    "        for i in range(1, len(instance['tokens'])):\n",
    "            if (instance['tokens'][i - 1].text.lower() in self.chem_mods or\n",
    "                    len(instance['tokens'][i - 1].text) < 2) \\\n",
    "                    and instance['tokens'][i].text == \"-\":\n",
    "                links[i] = 1\n",
    "\n",
    "        return links\n",
    "\n",
    "\n",
    "lf = HyphenatedPrefix()\n",
    "lf.apply(cdr_docs)\n",
    "\n",
    "\n",
    "class PostHyphen(LinkingRule):\n",
    "    def apply_instance(self, instance):\n",
    "        links = [0] * len(instance['tokens'])\n",
    "        for i in range(1, len(instance['tokens'])):\n",
    "            if instance['tokens'][i - 1].text == \"-\":\n",
    "                links[i] = 1\n",
    "\n",
    "        return links\n",
    "\n",
    "\n",
    "lf = PostHyphen()\n",
    "lf.apply(cdr_docs)\n",
    "\n",
    "\n",
    "dict_full = set()\n",
    "\n",
    "with open('../data/AutoNER_dicts/BC5CDR/dict_full.txt') as f:\n",
    "    for line in f.readlines():\n",
    "        tokens = cdr_reader.get_tokenizer()(line.strip())\n",
    "        term = tuple([str(x) for x in tokens])\n",
    "        if len(term) > 1:\n",
    "            dict_full.add(tuple(term))\n",
    "\n",
    "\n",
    "class ExtractedPhrase(LinkingRule):\n",
    "    def __init__(self, terms):\n",
    "        self.term_dict = {}\n",
    "\n",
    "        for term in terms:\n",
    "            term = [token.lower() for token in term]\n",
    "            if term[0] not in self.term_dict:\n",
    "                self.term_dict[term[0]] = []\n",
    "            self.term_dict[term[0]].append(term)\n",
    "\n",
    "        # Sorts the terms in decreasing order so that we match the longest\n",
    "        # first\n",
    "        for first_token in self.term_dict.keys():\n",
    "            to_sort = self.term_dict[first_token]\n",
    "            self.term_dict[first_token] = sorted(\n",
    "                to_sort, reverse=True, key=lambda x: len(x))\n",
    "\n",
    "    def apply_instance(self, instance):\n",
    "        tokens = [token.text.lower() for token in instance['tokens']]\n",
    "        links = [0] * len(instance['tokens'])\n",
    "\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            if tokens[i] in self.term_dict:\n",
    "                candidates = self.term_dict[tokens[i]]\n",
    "                for c in candidates:\n",
    "                    # Checks whether normalized AllenNLP tokens equal the list\n",
    "                    # of string tokens defining the term in the dictionary\n",
    "                    if i + len(c) <= len(tokens):\n",
    "                        equal = True\n",
    "                        for j in range(len(c)):\n",
    "                            if tokens[i + j] != c[j]:\n",
    "                                equal = False\n",
    "                                break\n",
    "\n",
    "                        # If tokens match, labels the instance tokens\n",
    "                        if equal:\n",
    "                            for j in range(i + 1, i + len(c)):\n",
    "                                links[j] = 1\n",
    "                            i = i + len(c) - 1\n",
    "                            break\n",
    "            i += 1\n",
    "\n",
    "        return links\n",
    "\n",
    "\n",
    "lf = ExtractedPhrase(dict_full)\n",
    "lf.apply(cdr_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_to_token_span(tokens: List[str],\n",
    "                      text: str,\n",
    "                      txt_spans: List[tuple]):\n",
    "    \"\"\"\n",
    "    Transfer text-domain spans to token-domain spans\n",
    "    :param tokens: tokens\n",
    "    :param text: text\n",
    "    :param txt_spans: text spans tuples: (start, end, ...)\n",
    "    :return: a list of transferred span tuples.\n",
    "    \"\"\"\n",
    "    token_indices = get_original_spans(tokens, text)\n",
    "    tgt_spans = list()\n",
    "    for txt_span in txt_spans:\n",
    "        spacy_start = txt_span[0]\n",
    "        spacy_end = txt_span[1]\n",
    "        start = None\n",
    "        end = None\n",
    "        for i, (s, e) in enumerate(token_indices):\n",
    "            if s <= spacy_start < e:\n",
    "                start = i\n",
    "            if s <= spacy_end <= e:\n",
    "                end = i + 1\n",
    "            if (start is not None) and (end is not None):\n",
    "                break\n",
    "        assert (start is not None) and (end is not None), ValueError(\"input spans out of scope\")\n",
    "        tgt_spans.append((start, end))\n",
    "    return tgt_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respan(src_tokens: List[str],\n",
    "           tgt_tokens: List[str],\n",
    "           src_span: List[tuple]):\n",
    "    \"\"\"\n",
    "    transfer original spans to target spans\n",
    "    :param src_tokens: source tokens\n",
    "    :param tgt_tokens: target tokens\n",
    "    :param src_span: a list of span tuples. The first element in the tuple\n",
    "    should be the start index and the second should be the end index\n",
    "    :return: a list of transferred span tuples.\n",
    "    \"\"\"\n",
    "    s2t, _ = get_alignments(src_tokens, tgt_tokens)\n",
    "    tgt_spans = list()\n",
    "    for spans in src_span:\n",
    "        start = s2t[spans[0]][0]\n",
    "        if spans[1] < len(s2t):\n",
    "            end = s2t[spans[1]-1][-1] + 1\n",
    "        else:\n",
    "            end = s2t[-1][-1]\n",
    "        if end == start:\n",
    "            end += 1\n",
    "        tgt_spans.append((start, end))\n",
    "\n",
    "    return tgt_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_span(labels: List[str],\n",
    "                  scheme: Optional[str] = 'BIO') -> dict:\n",
    "    \"\"\"\n",
    "    convert labels to spans\n",
    "    :param labels: a list of labels\n",
    "    :param scheme: labeling scheme, in ['BIO', 'BILOU'].\n",
    "    :return: labeled spans, a list of tuples (start_idx, end_idx, label)\n",
    "    \"\"\"\n",
    "    assert scheme in ['BIO', 'BILOU'], ValueError(\"unknown labeling scheme\")\n",
    "\n",
    "    labeled_spans = dict()\n",
    "    i = 0\n",
    "    while i < len(labels):\n",
    "        if labels[i] == 'O' or labels[i] == 'ABS':\n",
    "            i += 1\n",
    "            continue\n",
    "        else:\n",
    "            if scheme == 'BIO':\n",
    "                if labels[i][0] == 'B':\n",
    "                    start = i\n",
    "                    lb = labels[i][2:]\n",
    "                    i += 1\n",
    "                    try:\n",
    "                        while labels[i][0] == 'I':\n",
    "                            i += 1\n",
    "                        end = i\n",
    "                        labeled_spans[(start, end)] = lb\n",
    "                    except IndexError:\n",
    "                        end = i\n",
    "                        labeled_spans[(start, end)] = lb\n",
    "                        i += 1\n",
    "                # this should not happen\n",
    "                elif labels[i][0] == 'I':\n",
    "                    i += 1\n",
    "            elif scheme == 'BILOU':\n",
    "                if labels[i][0] == 'U':\n",
    "                    start = i\n",
    "                    end = i + 1\n",
    "                    lb = labels[i][2:]\n",
    "                    labeled_spans[(start, end)] = lb\n",
    "                    i += 1\n",
    "                elif labels[i][0] == 'B':\n",
    "                    start = i\n",
    "                    lb = labels[i][2:]\n",
    "                    i += 1\n",
    "                    try:\n",
    "                        while labels[i][0] != 'L':\n",
    "                            i += 1\n",
    "                        end = i\n",
    "                        labeled_spans[(start, end)] = lb\n",
    "                    except IndexError:\n",
    "                        end = i\n",
    "                        labeled_spans[(start, end)] = lb\n",
    "                        break\n",
    "                    i += 1\n",
    "                else:\n",
    "                    i += 1\n",
    "\n",
    "    return labeled_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bert_emb(sents: List[str],\n",
    "                   tokenizer,\n",
    "                   model,\n",
    "                   device: str):\n",
    "    bert_embs = list()\n",
    "    for i, sent in enumerate(sents):\n",
    "\n",
    "        joint_sent = ' '.join(sent)\n",
    "        bert_tokens = tokenizer.tokenize(joint_sent)\n",
    "\n",
    "        input_ids = torch.tensor([tokenizer.encode(joint_sent, add_special_tokens=True)], device=device)\n",
    "        # calculate BERT last layer embeddings\n",
    "        with torch.no_grad():\n",
    "            last_hidden_states = model(input_ids)[0].squeeze(0).to('cpu')\n",
    "            trunc_hidden_states = last_hidden_states[1:-1, :]\n",
    "\n",
    "        ori2bert, bert2ori = get_alignments(sent, bert_tokens)\n",
    "\n",
    "        emb_list = list()\n",
    "        for idx in ori2bert:\n",
    "            emb = trunc_hidden_states[idx, :]\n",
    "            emb_list.append(emb.mean(dim=0))\n",
    "\n",
    "        # TODO: using the embedding of [CLS] may not be the best idea\n",
    "        # It does not matter since that embedding is not used in the training\n",
    "        emb_list = [last_hidden_states[0, :]] + emb_list\n",
    "        bert_emb = torch.stack(emb_list)\n",
    "        bert_embs.append(bert_emb)\n",
    "    return bert_embs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct My dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = ['Chemical', 'Disease']\n",
    "LINK = 'LINK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2234123c769c420b938c4ffeac3cf382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea8252b8299b4699aaafd980926e26d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e2ed29e3ac94aafab7ff191663ffcb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_path = '../data/BC5CDR/CDR_TrainingSet.BioC.xml'\n",
    "dev_path = '../data/BC5CDR/CDR_DevelopmentSet.BioC.xml'\n",
    "test_path = '../data/BC5CDR/CDR_TestSet.BioC.xml'\n",
    "\n",
    "root = ElementTree.parse(train_path).getroot()\n",
    "xml_docs = root.findall(\"./document\")\n",
    "train_xml_sents = list()\n",
    "for xml_doc in tqdm(xml_docs):\n",
    "    xml_title = xml_doc.find(\"passage[infon='title']\")\n",
    "    xml_abstract = xml_doc.find(\"passage[infon='abstract']\")\n",
    "\n",
    "    title = xml_title.find('text').text\n",
    "    abstract = xml_abstract.find('text').text\n",
    "    train_xml_sents.append(title + \" \" + abstract)\n",
    "\n",
    "root = ElementTree.parse(dev_path).getroot()\n",
    "xml_docs = root.findall(\"./document\")\n",
    "dev_xml_sents = list()\n",
    "for xml_doc in tqdm(xml_docs):\n",
    "    xml_title = xml_doc.find(\"passage[infon='title']\")\n",
    "    xml_abstract = xml_doc.find(\"passage[infon='abstract']\")\n",
    "\n",
    "    title = xml_title.find('text').text\n",
    "    abstract = xml_abstract.find('text').text\n",
    "    dev_xml_sents.append(title + \" \" + abstract)\n",
    "\n",
    "root = ElementTree.parse(test_path).getroot()\n",
    "xml_docs = root.findall(\"./document\")\n",
    "test_xml_sents = list()\n",
    "for xml_doc in tqdm(xml_docs):\n",
    "    xml_title = xml_doc.find(\"passage[infon='title']\")\n",
    "    xml_abstract = xml_doc.find(\"passage[infon='abstract']\")\n",
    "\n",
    "    title = xml_title.find('text').text\n",
    "    abstract = xml_abstract.find('text').text\n",
    "    test_xml_sents.append(title + \" \" + abstract)\n",
    "\n",
    "xml_sents = train_xml_sents + dev_xml_sents + test_xml_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_token_list = list()\n",
    "src_anno_list = list()\n",
    "weak_anno_list = list()\n",
    "link_anno_list = list()\n",
    "\n",
    "allen_data = cdr_docs\n",
    "mapping_dict = {0:'O', 1:'I'}\n",
    "\n",
    "for src_txt, allen_annos in zip(xml_sents, allen_data):\n",
    "    \n",
    "    # handle the data read from the source text\n",
    "    src_tokens = word_tokenize(src_txt)\n",
    "    for i in range(len(src_tokens)):\n",
    "        if src_tokens[i] == r'``' or src_tokens[i] == r\"''\":\n",
    "            src_tokens[i] = r'\"'\n",
    "    \n",
    "    \n",
    "    allen_tokens = list(map(str, allen_annos['tokens']))\n",
    "    \n",
    "    src_labels = list(allen_annos['tags'])\n",
    "    pre_anno = 'O'\n",
    "    for i in range(len(src_labels)):\n",
    "        current_anno = src_labels[i]\n",
    "        if src_labels[i][0] == 'I':\n",
    "            if pre_anno[0] != 'I' and pre_anno[0] != 'B':\n",
    "                src_labels[i] = 'B-' + src_labels[i][2:]\n",
    "        pre_anno = current_anno\n",
    "    src_spans = label_to_span(src_labels)\n",
    "    src_spans_ = respan(allen_tokens, src_tokens, src_spans)\n",
    "    \n",
    "    src_annos = dict()\n",
    "    for span, lb in zip(src_spans_, src_spans.values()):\n",
    "        src_annos[span] = lb\n",
    "\n",
    "    src_token_list.append(src_tokens)\n",
    "    src_anno_list.append(src_annos)\n",
    "    \n",
    "    # handle the data constructed using Allennlp\n",
    "    weak_anno = dict()\n",
    "    \n",
    "    for k in allen_annos['WISER_LABELS']:\n",
    "        std_lbs = allen_annos['WISER_LABELS'][k][:]\n",
    "        \n",
    "        pre_anno = 'O'\n",
    "        for i in range(len(std_lbs)):\n",
    "            current_anno = std_lbs[i]\n",
    "            if std_lbs[i][0] == 'I':\n",
    "                if pre_anno[0] != 'I' and pre_anno[0] != 'B':\n",
    "                    std_lbs[i] = 'B-' + std_lbs[i][2:]\n",
    "            pre_anno = current_anno\n",
    "        weak_span = label_to_span(std_lbs)\n",
    "\n",
    "        src_weak_span = respan(allen_tokens, src_tokens, weak_span)\n",
    "        src_weak_anno = dict()\n",
    "        for span, lb in zip(src_weak_span, weak_span.values()):\n",
    "            src_weak_anno[span] = [(lb, 1.0)]\n",
    "            \n",
    "        weak_anno[k] = src_weak_anno\n",
    "    weak_anno_list.append(weak_anno)\n",
    "\n",
    "    \n",
    "    linked_dict = dict()\n",
    "    for src, entity_lbs in allen_annos['WISER_LINKS'].items():\n",
    "        entity_lbs = [mapping_dict[lb] for lb in entity_lbs]\n",
    "\n",
    "        pre_anno = 'O'\n",
    "        for i in range(len(entity_lbs)):\n",
    "            current_anno = entity_lbs[i]\n",
    "            if entity_lbs[i] == 'I':\n",
    "                if pre_anno != 'I':\n",
    "                    entity_lbs[i] = 'B-' + LINK\n",
    "                else:\n",
    "                    entity_lbs[i] = 'I-' + LINK\n",
    "            pre_anno = current_anno\n",
    "\n",
    "        entity_spans = label_to_span(entity_lbs)\n",
    "        complete_span = dict()\n",
    "        for (start, end), lb in entity_spans.items():\n",
    "            if start != 0:\n",
    "                start = start - 1\n",
    "            complete_span[(start, end)] = lb\n",
    "        src_link_span = respan(allen_tokens, src_tokens, complete_span)\n",
    "        linked_dict[src] = src_link_span\n",
    "    link_anno_list.append(linked_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_link_anno_list = list()\n",
    "for tag_anno, link_anno in zip(weak_anno_list, link_anno_list):\n",
    "    tag_spans = list()\n",
    "    for src, spans in tag_anno.items():\n",
    "        for k, v in spans.items():\n",
    "            tag_spans.append((set(range(k[0], k[1])), v[0][0]))\n",
    "    \n",
    "    link_entities = dict()\n",
    "    for src, spans in link_anno.items():\n",
    "        valid_spans = dict()\n",
    "        for span in spans:\n",
    "#             if span[1] - span[0] == 1:\n",
    "#                 continue\n",
    "            span_set = set(range(span[0], span[1]))\n",
    "            for tag_span, lb in tag_spans:\n",
    "                if span_set.intersection(tag_span):\n",
    "                    if span in valid_spans.keys():\n",
    "                        if lb not in valid_spans[span]:\n",
    "                            valid_spans[span].append(lb)\n",
    "                    else:\n",
    "                        valid_spans[span] = [lb]\n",
    "        valid_anno = dict()\n",
    "        for sp, lbs in valid_spans.items():\n",
    "            prob = 1/len(lbs)\n",
    "            valid_anno[sp] = [(lb, prob) for lb in lbs]\n",
    "        link_entities[src] = valid_anno\n",
    "    updated_link_anno_list.append(link_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_anno_list = list()\n",
    "for tag_anno, link_anno in zip(weak_anno_list, updated_link_anno_list):\n",
    "    comb_anno = dict()\n",
    "    for k, v in tag_anno.items():\n",
    "        comb_anno[k] = v\n",
    "    for k, v in link_anno.items():\n",
    "        comb_anno[k] = v\n",
    "    combined_anno_list.append(comb_anno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Bert Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "\n",
    "model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "standarized_sents = list()\n",
    "o2n_map = list()\n",
    "n=0\n",
    "for i, sents in enumerate(src_token_list):\n",
    "    joint_sent = ' '.join(sents)\n",
    "    len_bert_tokens = len(tokenizer.tokenize(joint_sent))\n",
    "    if len_bert_tokens >= 510:        \n",
    "        sts = sent_tokenize(joint_sent)\n",
    "        \n",
    "        sent_lens = list()\n",
    "        for st in sts:\n",
    "            sent_lens.append(len(word_tokenize(st)))\n",
    "        ends = [np.sum(sent_lens[:i]) for i in range(1, len(sent_lens)+1)]\n",
    "        \n",
    "        nearest_end_idx1 = np.argmin((np.array(ends) - len_bert_tokens / 3) ** 2)\n",
    "        nearest_end_idx2 = np.argmin((np.array(ends) - len_bert_tokens / 3 * 2) ** 2)\n",
    "        split_1 = sents[:ends[nearest_end_idx1]]\n",
    "        split_2 = sents[ends[nearest_end_idx1]:ends[nearest_end_idx2]]\n",
    "        split_3 = sents[ends[nearest_end_idx2]:]\n",
    "        standarized_sents.append(split_1)\n",
    "        standarized_sents.append(split_2)\n",
    "        standarized_sents.append(split_3)\n",
    "        o2n_map.append([n, n+1, n+2])\n",
    "        n += 3\n",
    "\n",
    "    else:\n",
    "        standarized_sents.append(sents)\n",
    "        o2n_map.append([n])\n",
    "        n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sents in enumerate(standarized_sents):\n",
    "    joint_sent = ' '.join(sents)\n",
    "    if len(tokenizer.tokenize(joint_sent)) >= 510:\n",
    "        print(i, len(sents), len(tokenizer.tokenize(joint_sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = model.to(device)\n",
    "embs = build_bert_emb(standarized_sents, tokenizer, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_embs = list()\n",
    "for o2n in o2n_map:\n",
    "    if len(o2n) == 1:\n",
    "        combined_embs.append(embs[o2n[0]])\n",
    "    else:\n",
    "        cat_emb = torch.cat([embs[o2n[0]], embs[o2n[1]][1:], embs[o2n[2]][1:]], dim=0)\n",
    "        combined_embs.append(cat_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for emb, sent in zip(combined_embs, src_token_list):\n",
    "    assert len(emb) == len(sent) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_token_list = src_token_list[1000:]\n",
    "test_anno_list = combined_anno_list[1000:]\n",
    "test_lb_list = src_anno_list[1000:]\n",
    "test_emb = combined_embs[1000:]\n",
    "\n",
    "train_token_list = src_token_list[:500]\n",
    "train_anno_list = combined_anno_list[:500]\n",
    "train_lb_list = src_anno_list[:500]\n",
    "train_emb = combined_embs[:500]\n",
    "\n",
    "dev_token_list = src_token_list[500:1000]\n",
    "dev_anno_list = combined_anno_list[500:1000]\n",
    "dev_lb_list = src_anno_list[500:1000]\n",
    "dev_emb = combined_embs[500:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = {\n",
    "    \"sentences\": train_token_list,\n",
    "    \"annotations\": train_anno_list,\n",
    "    \"labels\": train_lb_list\n",
    "}\n",
    "\n",
    "torch.save(train_data, f\"BC5CDR-linked-train.pt\")\n",
    "torch.save(train_emb, f\"BC5CDR-emb-train.pt\")\n",
    "\n",
    "dev_data = {\n",
    "    \"sentences\": dev_token_list,\n",
    "    \"annotations\": dev_anno_list,\n",
    "    \"labels\": dev_lb_list\n",
    "}\n",
    "\n",
    "torch.save(dev_data, f\"BC5CDR-linked-dev.pt\")\n",
    "torch.save(dev_emb, f\"BC5CDR-emb-dev.pt\")\n",
    "\n",
    "test_data = {\n",
    "    \"sentences\": test_token_list,\n",
    "    \"annotations\": test_anno_list,\n",
    "    \"labels\": test_lb_list\n",
    "}\n",
    "\n",
    "torch.save(test_data, f\"BC5CDR-linked-test.pt\")\n",
    "torch.save(test_emb, f\"BC5CDR-emb-test.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
